
==> Audit <==
|---------|-----------------|----------|-----------------------|---------|----------------------|----------------------|
| Command |      Args       | Profile  |         User          | Version |      Start Time      |       End Time       |
|---------|-----------------|----------|-----------------------|---------|----------------------|----------------------|
| start   |                 | minikube | LAPTOP-IBAITKM4\mkuzm | v1.34.0 | 20 Sep 24 23:30 CEST | 20 Sep 24 23:42 CEST |
| ip      |                 | minikube | LAPTOP-IBAITKM4\mkuzm | v1.34.0 | 20 Sep 24 23:57 CEST | 20 Sep 24 23:57 CEST |
| service | nodeapp-service | minikube | LAPTOP-IBAITKM4\mkuzm | v1.34.0 | 20 Sep 24 23:58 CEST |                      |
| service | nodeapp-service | minikube | LAPTOP-IBAITKM4\mkuzm | v1.34.0 | 21 Sep 24 00:01 CEST |                      |
| service | nodeapp-service | minikube | LAPTOP-IBAITKM4\mkuzm | v1.34.0 | 21 Sep 24 00:02 CEST |                      |
|---------|-----------------|----------|-----------------------|---------|----------------------|----------------------|


==> Last Start <==
Log file created at: 2024/09/20 23:30:05
Running on machine: LAPTOP-IBAITKM4
Binary: Built with gc go1.22.5 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0920 23:30:05.139509   34172 out.go:345] Setting OutFile to fd 104 ...
I0920 23:30:05.140047   34172 out.go:397] isatty.IsTerminal(104) = true
I0920 23:30:05.140047   34172 out.go:358] Setting ErrFile to fd 108...
I0920 23:30:05.140047   34172 out.go:397] isatty.IsTerminal(108) = true
W0920 23:30:05.206841   34172 root.go:314] Error reading config file at C:\Users\mkuzm\.minikube\config\config.json: open C:\Users\mkuzm\.minikube\config\config.json: The system cannot find the path specified.
I0920 23:30:05.251721   34172 out.go:352] Setting JSON to false
I0920 23:30:05.276193   34172 start.go:129] hostinfo: {"hostname":"LAPTOP-IBAITKM4","uptime":444260,"bootTime":1726423545,"procs":399,"os":"windows","platform":"Microsoft Windows 11 Home Single Language","platformFamily":"Standalone Workstation","platformVersion":"10.0.22631.4169 Build 22631.4169","kernelVersion":"10.0.22631.4169 Build 22631.4169","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"7c4fbf33-b8d4-4c1d-8980-35ca8675e9dc"}
W0920 23:30:05.276193   34172 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0920 23:30:05.286794   34172 out.go:177] 😄  minikube v1.34.0 on Microsoft Windows 11 Home Single Language 10.0.22631.4169 Build 22631.4169
I0920 23:30:05.296058   34172 notify.go:220] Checking for updates...
I0920 23:30:05.296623   34172 driver.go:394] Setting default libvirt URI to qemu:///system
I0920 23:30:05.296623   34172 global.go:112] Querying for installed drivers using PATH=C:\Users\mkuzm\AppData\Roaming\npm;C:\Program Files\Common Files\Oracle\Java\javapath;C:\Program Files (x86)\Common Files\Oracle\Java\java8path;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\WINDOWS\System32\OpenSSH\;C:\Program Files\Java\jdk-19\bin\;C:\Program Files\Git\cmd;C:\Program Files\dotnet\;C:\Program Files\Microsoft SQL Server\150\Tools\Binn\;C:\Program Files\Docker\Docker\resources\bin;C:\Program Files\Microsoft SQL Server\Client SDK\ODBC\170\Tools\Binn\;C:\Program Files (x86)\Microsoft SQL Server\160\DTS\Binn\;C:\Program Files\Azure Data Studio\bin;C:\ProgramData\chocolatey\bin;;C:\Program Files\HP\HP One Agent;C:\Program Files\nodejs\;C:\Users\mkuzm\AppData\Local\Programs\Python\Python312;C:\Users\mkuzm\AppData\Local\Programs\Python\Python312\Lib\site-packages;C:\Users\mkuzm\AppData\Local\Microsoft\WindowsApps;C:\Users\mkuzm\AppData\Local\Programs\Microsoft VS Code\bin;C:\ProgramData\chocolatey\bin\kubectl.exe;C:\Users\mkuzm\AppData\Local\GitHubDesktop\bin;C:\Users\mkuzm\.dotnet\tools;C:\Program Files\Azure Data Studio\bin;C:\Users\mkuzm\AppData\Roaming\npm;C:\Users\mkuzm\AppData\Local\Programs\Python\Python312\Scripts;
W0920 23:30:05.297810   34172 preload.go:293] Failed to list preload files: open C:\Users\mkuzm\.minikube\cache\preloaded-tarball: The system cannot find the file specified.
I0920 23:30:12.608090   34172 global.go:133] hyperv default: true priority: 8, state: {Installed:true Healthy:false Running:false NeedsImprovement:false Error:Hyper-V requires Administrator privileges Reason: Fix:Right-click the PowerShell icon and select Run as Administrator to open PowerShell in elevated mode. Doc: Version:}
I0920 23:30:12.620383   34172 global.go:133] qemu2 default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-x86_64": executable file not found in %PATH% Reason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
I0920 23:30:12.648024   34172 global.go:133] virtualbox default: true priority: 6, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:unable to find VBoxManage in $PATH Reason: Fix:Install VirtualBox Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}
I0920 23:30:12.660153   34172 global.go:133] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in %PATH% Reason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I0920 23:30:12.952400   34172 docker.go:123] docker version: linux-25.0.3:Docker Desktop 4.28.0 (139021)
I0920 23:30:12.960952   34172 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0920 23:30:14.787793   34172 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1.8268412s)
I0920 23:30:14.798439   34172 info.go:266] docker info: {ID:ef101bb8-8285-4e34-b1be-310b90ed9b32 Containers:3 ContainersRunning:3 ContainersPaused:0 ContainersStopped:0 Images:3 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:74 OomKillDisable:true NGoroutines:104 SystemTime:2024-09-20 21:30:14.620650194 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:3840749568 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:25.0.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.1-desktop.4] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.24.6-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container. Vendor:Docker Inc. Version:0.0.24] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.22] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.0.1] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.5.0]] Warnings:<nil>}}
I0920 23:30:14.798439   34172 global.go:133] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0920 23:30:14.823673   34172 global.go:133] podman default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in %PATH% Reason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I0920 23:30:14.823673   34172 global.go:133] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0920 23:30:14.826807   34172 driver.go:316] not recommending "ssh" due to default: false
I0920 23:30:14.826807   34172 driver.go:311] not recommending "hyperv" due to health: Hyper-V requires Administrator privileges
I0920 23:30:14.826807   34172 driver.go:351] Picked: docker
I0920 23:30:14.826807   34172 driver.go:352] Alternatives: [ssh]
I0920 23:30:14.826807   34172 driver.go:353] Rejects: [hyperv qemu2 virtualbox vmware podman]
I0920 23:30:14.831530   34172 out.go:177] ✨  Automatically selected the docker driver
I0920 23:30:14.843806   34172 start.go:297] selected driver: docker
I0920 23:30:14.843806   34172 start.go:901] validating driver "docker" against <nil>
I0920 23:30:14.843806   34172 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0920 23:30:14.861500   34172 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0920 23:30:16.278694   34172 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1.4171931s)
I0920 23:30:16.280859   34172 info.go:266] docker info: {ID:ef101bb8-8285-4e34-b1be-310b90ed9b32 Containers:3 ContainersRunning:3 ContainersPaused:0 ContainersStopped:0 Images:3 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:74 OomKillDisable:true NGoroutines:104 SystemTime:2024-09-20 21:30:16.227619056 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:3840749568 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:25.0.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.1-desktop.4] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.24.6-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container. Vendor:Docker Inc. Version:0.0.24] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.22] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.0.1] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.5.0]] Warnings:<nil>}}
I0920 23:30:16.281466   34172 start_flags.go:310] no existing cluster config was found, will generate one from the flags 
I0920 23:30:16.401480   34172 start_flags.go:393] Using suggested 2200MB memory alloc based on sys=7540MB, container=3662MB
I0920 23:30:16.403766   34172 start_flags.go:929] Wait components to verify : map[apiserver:true system_pods:true]
I0920 23:30:16.407063   34172 out.go:177] 📌  Using Docker Desktop driver with root privileges
I0920 23:30:16.412417   34172 cni.go:84] Creating CNI manager for ""
I0920 23:30:16.412975   34172 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0920 23:30:16.412975   34172 start_flags.go:319] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0920 23:30:16.413603   34172 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\mkuzm:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0920 23:30:16.416339   34172 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0920 23:30:16.425770   34172 cache.go:121] Beginning downloading kic base image for docker with docker
I0920 23:30:16.431067   34172 out.go:177] 🚜  Pulling base image v0.0.45 ...
I0920 23:30:16.434914   34172 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I0920 23:30:16.434914   34172 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local docker daemon
I0920 23:30:16.611909   34172 preload.go:118] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.31.0/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4
I0920 23:30:16.611909   34172 cache.go:56] Caching tarball of preloaded images
I0920 23:30:16.612437   34172 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I0920 23:30:16.615296   34172 out.go:177] 💾  Downloading Kubernetes v1.31.0 preload ...
I0920 23:30:16.619627   34172 preload.go:236] getting checksum for preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4 ...
I0920 23:30:16.689323   34172 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 to local cache
I0920 23:30:16.691685   34172 localpath.go:151] windows sanitize: C:\Users\mkuzm\.minikube\cache\kic\amd64\kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\mkuzm\.minikube\cache\kic\amd64\kicbase_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
I0920 23:30:16.692193   34172 localpath.go:151] windows sanitize: C:\Users\mkuzm\.minikube\cache\kic\amd64\kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\mkuzm\.minikube\cache\kic\amd64\kicbase_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
I0920 23:30:16.692734   34172 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory
I0920 23:30:16.693270   34172 image.go:148] Writing gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 to local cache
I0920 23:30:16.887101   34172 download.go:107] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.31.0/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4?checksum=md5:2dd98f97b896d7a4f012ee403b477cc8 -> C:\Users\mkuzm\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4
I0920 23:32:03.548824   34172 preload.go:247] saving checksum for preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4 ...
I0920 23:32:03.556077   34172 preload.go:254] verifying checksum of C:\Users\mkuzm\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4 ...
I0920 23:32:04.765522   34172 cache.go:59] Finished verifying existence of preloaded tar for v1.31.0 on docker
I0920 23:32:04.821306   34172 profile.go:143] Saving config to C:\Users\mkuzm\.minikube\profiles\minikube\config.json ...
I0920 23:32:04.826825   34172 lock.go:35] WriteFile acquiring C:\Users\mkuzm\.minikube\profiles\minikube\config.json: {Name:mk0666b3ef08befb63dde21543df7a0866ed02b7 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0920 23:32:43.546939   34172 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 as a tarball
I0920 23:32:43.548023   34172 cache.go:162] Loading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from local cache
I0920 23:32:43.552406   34172 localpath.go:151] windows sanitize: C:\Users\mkuzm\.minikube\cache\kic\amd64\kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\mkuzm\.minikube\cache\kic\amd64\kicbase_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
I0920 23:34:52.133581   34172 cache.go:164] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from cached tarball
I0920 23:34:52.155836   34172 cache.go:194] Successfully downloaded all kic artifacts
I0920 23:34:52.266913   34172 start.go:360] acquireMachinesLock for minikube: {Name:mkad9cf2e6a7cfbb08bfb24292ce27cdb3328ca9 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0920 23:34:52.283895   34172 start.go:364] duration metric: took 11.2756ms to acquireMachinesLock for "minikube"
I0920 23:34:52.339719   34172 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\mkuzm:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0920 23:34:52.488685   34172 start.go:125] createHost starting for "" (driver="docker")
I0920 23:34:52.699756   34172 out.go:235] 🔥  Creating docker container (CPUs=2, Memory=2200MB) ...
I0920 23:34:52.740727   34172 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0920 23:34:52.741998   34172 client.go:168] LocalClient.Create starting
I0920 23:34:52.750550   34172 main.go:141] libmachine: Creating CA: C:\Users\mkuzm\.minikube\certs\ca.pem
I0920 23:34:53.214669   34172 main.go:141] libmachine: Creating client certificate: C:\Users\mkuzm\.minikube\certs\cert.pem
I0920 23:34:53.496159   34172 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0920 23:34:53.779103   34172 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0920 23:34:53.786970   34172 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I0920 23:34:53.786970   34172 cli_runner.go:164] Run: docker network inspect minikube
W0920 23:34:53.977186   34172 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0920 23:34:53.977186   34172 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0920 23:34:53.977186   34172 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0920 23:34:53.987803   34172 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0920 23:34:54.309222   34172 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc001570a20}
I0920 23:34:54.310300   34172 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0920 23:34:54.324150   34172 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0920 23:34:55.721382   34172 cli_runner.go:217] Completed: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube: (1.3972321s)
I0920 23:34:55.721382   34172 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0920 23:34:55.722452   34172 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0920 23:34:55.739209   34172 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0920 23:34:55.972045   34172 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0920 23:34:56.226652   34172 oci.go:103] Successfully created a docker volume minikube
I0920 23:34:56.237460   34172 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 -d /var/lib
I0920 23:35:15.349313   34172 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 -d /var/lib: (19.1118535s)
I0920 23:35:15.350145   34172 oci.go:107] Successfully prepared a docker volume minikube
I0920 23:35:15.352320   34172 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I0920 23:35:15.356793   34172 kic.go:194] Starting extracting preloaded images to volume ...
I0920 23:35:15.368888   34172 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\mkuzm\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 -I lz4 -xf /preloaded.tar -C /extractDir
I0920 23:36:58.438467   34172 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\mkuzm\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 -I lz4 -xf /preloaded.tar -C /extractDir: (1m43.0537119s)
I0920 23:36:58.443352   34172 kic.go:203] duration metric: took 1m43.0878037s to extract preloaded images to volume ...
I0920 23:36:58.924571   34172 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0920 23:37:01.396861   34172 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (2.4722901s)
I0920 23:37:01.420218   34172 info.go:266] docker info: {ID:ef101bb8-8285-4e34-b1be-310b90ed9b32 Containers:3 ContainersRunning:3 ContainersPaused:0 ContainersStopped:0 Images:4 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:83 OomKillDisable:true NGoroutines:110 SystemTime:2024-09-20 21:37:01.275990301 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:3840749568 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:25.0.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.1-desktop.4] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.24.6-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container. Vendor:Docker Inc. Version:0.0.24] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.22] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.0.1] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.5.0]] Warnings:<nil>}}
I0920 23:37:01.438052   34172 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0920 23:37:02.159641   34172 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2200mb --memory-swap=2200mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85
I0920 23:37:10.973741   34172 cli_runner.go:217] Completed: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2200mb --memory-swap=2200mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85: (8.809612s)
I0920 23:37:11.003149   34172 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0920 23:37:11.460680   34172 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0920 23:37:12.320255   34172 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0920 23:37:13.346611   34172 cli_runner.go:217] Completed: docker exec minikube stat /var/lib/dpkg/alternatives/iptables: (1.0263564s)
I0920 23:37:13.347699   34172 oci.go:144] the created container "minikube" has a running status.
I0920 23:37:13.349895   34172 kic.go:225] Creating ssh key for kic: C:\Users\mkuzm\.minikube\machines\minikube\id_rsa...
I0920 23:37:13.788191   34172 kic_runner.go:191] docker (temp): C:\Users\mkuzm\.minikube\machines\minikube\id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0920 23:37:14.335312   34172 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0920 23:37:14.710177   34172 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0920 23:37:14.710177   34172 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0920 23:37:15.947134   34172 kic_runner.go:123] Done: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]: (1.236289s)
I0920 23:37:15.976264   34172 kic.go:265] ensuring only current user has permissions to key file located at : C:\Users\mkuzm\.minikube\machines\minikube\id_rsa...
W0920 23:37:24.134491   34172 kic.go:271] unable to determine current user's SID. minikube tunnel may not work.
I0920 23:37:24.168004   34172 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0920 23:37:24.564314   34172 machine.go:93] provisionDockerMachine start ...
I0920 23:37:24.592174   34172 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0920 23:37:25.018010   34172 main.go:141] libmachine: Using SSH client type: native
I0920 23:37:25.034785   34172 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x106c9c0] 0x106f5a0 <nil>  [] 0s} 127.0.0.1 54829 <nil> <nil>}
I0920 23:37:25.036813   34172 main.go:141] libmachine: About to run SSH command:
hostname
I0920 23:37:26.692429   34172 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0920 23:37:26.699127   34172 ubuntu.go:169] provisioning hostname "minikube"
I0920 23:37:26.716368   34172 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0920 23:37:27.155490   34172 main.go:141] libmachine: Using SSH client type: native
I0920 23:37:27.155820   34172 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x106c9c0] 0x106f5a0 <nil>  [] 0s} 127.0.0.1 54829 <nil> <nil>}
I0920 23:37:27.156330   34172 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0920 23:37:27.938313   34172 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0920 23:37:27.970778   34172 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0920 23:37:28.429263   34172 main.go:141] libmachine: Using SSH client type: native
I0920 23:37:28.454071   34172 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x106c9c0] 0x106f5a0 <nil>  [] 0s} 127.0.0.1 54829 <nil> <nil>}
I0920 23:37:28.454071   34172 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0920 23:37:28.829784   34172 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0920 23:37:28.836803   34172 ubuntu.go:175] set auth options {CertDir:C:\Users\mkuzm\.minikube CaCertPath:C:\Users\mkuzm\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\mkuzm\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\mkuzm\.minikube\machines\server.pem ServerKeyPath:C:\Users\mkuzm\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\mkuzm\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\mkuzm\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\mkuzm\.minikube}
I0920 23:37:28.840008   34172 ubuntu.go:177] setting up certificates
I0920 23:37:28.840701   34172 provision.go:84] configureAuth start
I0920 23:37:28.865791   34172 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0920 23:37:29.176026   34172 provision.go:143] copyHostCerts
I0920 23:37:29.184227   34172 exec_runner.go:151] cp: C:\Users\mkuzm\.minikube\certs\ca.pem --> C:\Users\mkuzm\.minikube/ca.pem (1074 bytes)
I0920 23:37:29.190652   34172 exec_runner.go:151] cp: C:\Users\mkuzm\.minikube\certs\cert.pem --> C:\Users\mkuzm\.minikube/cert.pem (1119 bytes)
I0920 23:37:29.195097   34172 exec_runner.go:151] cp: C:\Users\mkuzm\.minikube\certs\key.pem --> C:\Users\mkuzm\.minikube/key.pem (1679 bytes)
I0920 23:37:29.201126   34172 provision.go:117] generating server cert: C:\Users\mkuzm\.minikube\machines\server.pem ca-key=C:\Users\mkuzm\.minikube\certs\ca.pem private-key=C:\Users\mkuzm\.minikube\certs\ca-key.pem org=mkuzm.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0920 23:37:30.118323   34172 provision.go:177] copyRemoteCerts
I0920 23:37:30.141046   34172 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0920 23:37:30.151321   34172 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0920 23:37:30.377053   34172 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54829 SSHKeyPath:C:\Users\mkuzm\.minikube\machines\minikube\id_rsa Username:docker}
I0920 23:37:31.300510   34172 ssh_runner.go:235] Completed: sudo mkdir -p /etc/docker /etc/docker /etc/docker: (1.1594644s)
I0920 23:37:31.302939   34172 ssh_runner.go:362] scp C:\Users\mkuzm\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0920 23:37:31.455914   34172 ssh_runner.go:362] scp C:\Users\mkuzm\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0920 23:37:31.566755   34172 ssh_runner.go:362] scp C:\Users\mkuzm\.minikube\machines\server.pem --> /etc/docker/server.pem (1176 bytes)
I0920 23:37:31.862857   34172 provision.go:87] duration metric: took 3.0029824s to configureAuth
I0920 23:37:32.095527   34172 ubuntu.go:193] setting minikube options for container-runtime
I0920 23:37:32.341270   34172 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I0920 23:37:32.360661   34172 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0920 23:37:33.467149   34172 cli_runner.go:217] Completed: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube: (1.0951019s)
I0920 23:37:33.661558   34172 main.go:141] libmachine: Using SSH client type: native
I0920 23:37:33.773190   34172 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x106c9c0] 0x106f5a0 <nil>  [] 0s} 127.0.0.1 54829 <nil> <nil>}
I0920 23:37:33.773190   34172 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0920 23:37:37.140542   34172 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0920 23:37:37.141058   34172 ubuntu.go:71] root file system type: overlay
I0920 23:37:37.153623   34172 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0920 23:37:37.167241   34172 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0920 23:37:37.495355   34172 main.go:141] libmachine: Using SSH client type: native
I0920 23:37:37.497505   34172 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x106c9c0] 0x106f5a0 <nil>  [] 0s} 127.0.0.1 54829 <nil> <nil>}
I0920 23:37:37.497505   34172 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0920 23:37:38.745798   34172 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0920 23:37:38.767382   34172 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0920 23:37:39.085228   34172 main.go:141] libmachine: Using SSH client type: native
I0920 23:37:39.085228   34172 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x106c9c0] 0x106f5a0 <nil>  [] 0s} 127.0.0.1 54829 <nil> <nil>}
I0920 23:37:39.085783   34172 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0920 23:37:47.387296   34172 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-08-27 14:13:43.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2024-09-20 21:37:38.718659106 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0920 23:37:47.387296   34172 machine.go:96] duration metric: took 22.8229816s to provisionDockerMachine
I0920 23:37:47.388082   34172 client.go:171] duration metric: took 2m54.6460843s to LocalClient.Create
I0920 23:37:47.390016   34172 start.go:167] duration metric: took 2m54.6481279s to libmachine.API.Create "minikube"
I0920 23:37:47.391279   34172 start.go:293] postStartSetup for "minikube" (driver="docker")
I0920 23:37:47.391279   34172 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0920 23:37:47.422861   34172 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0920 23:37:47.435394   34172 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0920 23:37:47.739652   34172 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54829 SSHKeyPath:C:\Users\mkuzm\.minikube\machines\minikube\id_rsa Username:docker}
I0920 23:37:48.145029   34172 ssh_runner.go:195] Run: cat /etc/os-release
I0920 23:37:48.195574   34172 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0920 23:37:48.195574   34172 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0920 23:37:48.195574   34172 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0920 23:37:48.195574   34172 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0920 23:37:48.196280   34172 filesync.go:126] Scanning C:\Users\mkuzm\.minikube\addons for local assets ...
I0920 23:37:48.203424   34172 filesync.go:126] Scanning C:\Users\mkuzm\.minikube\files for local assets ...
I0920 23:37:48.205291   34172 start.go:296] duration metric: took 813.3832ms for postStartSetup
I0920 23:37:48.298046   34172 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0920 23:37:48.676553   34172 profile.go:143] Saving config to C:\Users\mkuzm\.minikube\profiles\minikube\config.json ...
I0920 23:37:48.711723   34172 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0920 23:37:48.719867   34172 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0920 23:37:49.175210   34172 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54829 SSHKeyPath:C:\Users\mkuzm\.minikube\machines\minikube\id_rsa Username:docker}
I0920 23:37:49.713176   34172 ssh_runner.go:235] Completed: sh -c "df -h /var | awk 'NR==2{print $5}'": (1.0014529s)
I0920 23:37:49.734105   34172 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0920 23:37:49.754600   34172 start.go:128] duration metric: took 2m57.2649047s to createHost
I0920 23:37:49.755111   34172 start.go:83] releasing machines lock for "minikube", held for 2m57.4712156s
I0920 23:37:49.765200   34172 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0920 23:37:50.016018   34172 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I0920 23:37:50.026002   34172 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0920 23:37:50.036527   34172 ssh_runner.go:195] Run: cat /version.json
I0920 23:37:50.043433   34172 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0920 23:37:50.350898   34172 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54829 SSHKeyPath:C:\Users\mkuzm\.minikube\machines\minikube\id_rsa Username:docker}
I0920 23:37:50.367182   34172 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54829 SSHKeyPath:C:\Users\mkuzm\.minikube\machines\minikube\id_rsa Username:docker}
W0920 23:37:50.590023   34172 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I0920 23:37:50.606811   34172 ssh_runner.go:195] Run: systemctl --version
I0920 23:37:50.642388   34172 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0920 23:37:50.716731   34172 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0920 23:37:50.767900   34172 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0920 23:37:50.791120   34172 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0920 23:37:50.941354   34172 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0920 23:37:50.942480   34172 start.go:495] detecting cgroup driver to use...
I0920 23:37:50.943298   34172 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0920 23:37:50.949701   34172 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0920 23:37:51.185257   34172 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0920 23:37:51.393661   34172 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0920 23:37:51.610768   34172 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0920 23:37:51.634222   34172 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0920 23:37:51.809327   34172 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0920 23:37:51.897924   34172 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0920 23:37:51.987822   34172 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
W0920 23:37:52.024123   34172 out.go:270] ❗  Failing to connect to https://registry.k8s.io/ from both inside the minikube container and host machine
W0920 23:37:52.025284   34172 out.go:270] 💡  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0920 23:37:52.036495   34172 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0920 23:37:52.090485   34172 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0920 23:37:52.157144   34172 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0920 23:37:52.199685   34172 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0920 23:37:52.303641   34172 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0920 23:37:52.371406   34172 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0920 23:37:52.432430   34172 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0920 23:37:52.779544   34172 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0920 23:37:53.741073   34172 start.go:495] detecting cgroup driver to use...
I0920 23:37:53.741073   34172 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0920 23:37:53.795448   34172 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0920 23:37:53.875162   34172 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0920 23:37:53.897284   34172 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0920 23:37:54.010745   34172 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0920 23:37:54.291279   34172 ssh_runner.go:195] Run: which cri-dockerd
I0920 23:37:54.314804   34172 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0920 23:37:54.337638   34172 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0920 23:37:54.450041   34172 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0920 23:37:54.856138   34172 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0920 23:37:55.264882   34172 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0920 23:37:55.284176   34172 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0920 23:37:55.437435   34172 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0920 23:38:00.090372   34172 ssh_runner.go:235] Completed: sudo systemctl daemon-reload: (4.6524277s)
I0920 23:38:00.102978   34172 ssh_runner.go:195] Run: sudo systemctl restart docker
I0920 23:38:04.642253   34172 ssh_runner.go:235] Completed: sudo systemctl restart docker: (4.539275s)
I0920 23:38:04.656058   34172 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0920 23:38:04.706344   34172 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0920 23:38:04.760122   34172 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0920 23:38:05.118597   34172 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0920 23:38:06.268955   34172 ssh_runner.go:235] Completed: sudo systemctl enable cri-docker.socket: (1.150358s)
I0920 23:38:06.284456   34172 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0920 23:38:06.584572   34172 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0920 23:38:06.630225   34172 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0920 23:38:06.676787   34172 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0920 23:38:06.965568   34172 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0920 23:38:09.263813   34172 ssh_runner.go:235] Completed: sudo systemctl restart cri-docker.service: (2.297706s)
I0920 23:38:09.263813   34172 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0920 23:38:09.282595   34172 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0920 23:38:09.308824   34172 start.go:563] Will wait 60s for crictl version
I0920 23:38:09.324876   34172 ssh_runner.go:195] Run: which crictl
I0920 23:38:09.367733   34172 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0920 23:38:13.037960   34172 ssh_runner.go:235] Completed: sudo /usr/bin/crictl version: (3.6702264s)
I0920 23:38:13.037960   34172 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.2.0
RuntimeApiVersion:  v1
I0920 23:38:13.048872   34172 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0920 23:38:15.434490   34172 ssh_runner.go:235] Completed: docker version --format {{.Server.Version}}: (2.3856184s)
I0920 23:38:15.450531   34172 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0920 23:38:16.003171   34172 out.go:235] 🐳  Preparing Kubernetes v1.31.0 on Docker 27.2.0 ...
I0920 23:38:16.033291   34172 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0920 23:38:17.282858   34172 cli_runner.go:217] Completed: docker exec -t minikube dig +short host.docker.internal: (1.249567s)
I0920 23:38:17.283440   34172 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0920 23:38:17.303939   34172 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0920 23:38:17.335865   34172 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0920 23:38:17.404682   34172 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0920 23:38:17.620085   34172 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\mkuzm:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0920 23:38:17.623337   34172 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I0920 23:38:17.630466   34172 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0920 23:38:17.885514   34172 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0920 23:38:17.885514   34172 docker.go:615] Images already preloaded, skipping extraction
I0920 23:38:17.894559   34172 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0920 23:38:18.050252   34172 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0920 23:38:18.050880   34172 cache_images.go:84] Images are preloaded, skipping loading
I0920 23:38:18.051387   34172 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.31.0 docker true true} ...
I0920 23:38:18.055232   34172 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.31.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0920 23:38:18.062094   34172 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0920 23:38:19.171761   34172 ssh_runner.go:235] Completed: docker info --format {{.CgroupDriver}}: (1.1096666s)
I0920 23:38:19.178461   34172 cni.go:84] Creating CNI manager for ""
I0920 23:38:19.178461   34172 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0920 23:38:19.178461   34172 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0920 23:38:19.179527   34172 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.31.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0920 23:38:19.186441   34172 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.31.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0920 23:38:19.207655   34172 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.31.0
I0920 23:38:19.240308   34172 binaries.go:44] Found k8s binaries, skipping transfer
I0920 23:38:19.256079   34172 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0920 23:38:19.289057   34172 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0920 23:38:19.353909   34172 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0920 23:38:19.401540   34172 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I0920 23:38:19.511473   34172 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0920 23:38:19.519790   34172 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0920 23:38:19.603001   34172 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0920 23:38:19.911912   34172 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0920 23:38:19.953984   34172 certs.go:68] Setting up C:\Users\mkuzm\.minikube\profiles\minikube for IP: 192.168.49.2
I0920 23:38:19.953984   34172 certs.go:194] generating shared ca certs ...
I0920 23:38:19.955147   34172 certs.go:226] acquiring lock for ca certs: {Name:mk4bf8aa7822946e97f83fab95445e3f13f119c0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0920 23:38:19.958479   34172 certs.go:240] generating "minikubeCA" ca cert: C:\Users\mkuzm\.minikube\ca.key
I0920 23:38:20.092535   34172 crypto.go:156] Writing cert to C:\Users\mkuzm\.minikube\ca.crt ...
I0920 23:38:20.092535   34172 lock.go:35] WriteFile acquiring C:\Users\mkuzm\.minikube\ca.crt: {Name:mk08db39aba0ae4b2e26e716a0a4d536f5b085e7 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0920 23:38:20.093534   34172 crypto.go:164] Writing key to C:\Users\mkuzm\.minikube\ca.key ...
I0920 23:38:20.093534   34172 lock.go:35] WriteFile acquiring C:\Users\mkuzm\.minikube\ca.key: {Name:mk7ee8470e463dc53dbe0eefc68dfebe23d88755 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0920 23:38:20.094535   34172 certs.go:240] generating "proxyClientCA" ca cert: C:\Users\mkuzm\.minikube\proxy-client-ca.key
I0920 23:38:20.371887   34172 crypto.go:156] Writing cert to C:\Users\mkuzm\.minikube\proxy-client-ca.crt ...
I0920 23:38:20.371887   34172 lock.go:35] WriteFile acquiring C:\Users\mkuzm\.minikube\proxy-client-ca.crt: {Name:mk3be968fdf6e4cf9984a52fbe7ede3fe6a36018 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0920 23:38:20.408337   34172 crypto.go:164] Writing key to C:\Users\mkuzm\.minikube\proxy-client-ca.key ...
I0920 23:38:20.408337   34172 lock.go:35] WriteFile acquiring C:\Users\mkuzm\.minikube\proxy-client-ca.key: {Name:mk2021dbf6d16e313e2a5dcf1abb0ad4d9172ea6 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0920 23:38:20.431204   34172 certs.go:256] generating profile certs ...
I0920 23:38:20.437112   34172 certs.go:363] generating signed profile cert for "minikube-user": C:\Users\mkuzm\.minikube\profiles\minikube\client.key
I0920 23:38:20.438862   34172 crypto.go:68] Generating cert C:\Users\mkuzm\.minikube\profiles\minikube\client.crt with IP's: []
I0920 23:38:20.902009   34172 crypto.go:156] Writing cert to C:\Users\mkuzm\.minikube\profiles\minikube\client.crt ...
I0920 23:38:20.902009   34172 lock.go:35] WriteFile acquiring C:\Users\mkuzm\.minikube\profiles\minikube\client.crt: {Name:mk67d37f411c23ab3fa91dbf54428bbbfd4a3199 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0920 23:38:20.903267   34172 crypto.go:164] Writing key to C:\Users\mkuzm\.minikube\profiles\minikube\client.key ...
I0920 23:38:20.903267   34172 lock.go:35] WriteFile acquiring C:\Users\mkuzm\.minikube\profiles\minikube\client.key: {Name:mkf9bdd3008e6281ec3d294fe2356ea8d710325c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0920 23:38:20.907830   34172 certs.go:363] generating signed profile cert for "minikube": C:\Users\mkuzm\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0920 23:38:20.907830   34172 crypto.go:68] Generating cert C:\Users\mkuzm\.minikube\profiles\minikube\apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0920 23:38:21.048665   34172 crypto.go:156] Writing cert to C:\Users\mkuzm\.minikube\profiles\minikube\apiserver.crt.7fb57e3c ...
I0920 23:38:21.048665   34172 lock.go:35] WriteFile acquiring C:\Users\mkuzm\.minikube\profiles\minikube\apiserver.crt.7fb57e3c: {Name:mkaa7566ef2809af8c7552b8d9de61fc6da5b647 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0920 23:38:21.051197   34172 crypto.go:164] Writing key to C:\Users\mkuzm\.minikube\profiles\minikube\apiserver.key.7fb57e3c ...
I0920 23:38:21.051197   34172 lock.go:35] WriteFile acquiring C:\Users\mkuzm\.minikube\profiles\minikube\apiserver.key.7fb57e3c: {Name:mk83fad4add6894a72b8b892b84746bcebaec82e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0920 23:38:21.053003   34172 certs.go:381] copying C:\Users\mkuzm\.minikube\profiles\minikube\apiserver.crt.7fb57e3c -> C:\Users\mkuzm\.minikube\profiles\minikube\apiserver.crt
I0920 23:38:21.056052   34172 certs.go:385] copying C:\Users\mkuzm\.minikube\profiles\minikube\apiserver.key.7fb57e3c -> C:\Users\mkuzm\.minikube\profiles\minikube\apiserver.key
I0920 23:38:21.058046   34172 certs.go:363] generating signed profile cert for "aggregator": C:\Users\mkuzm\.minikube\profiles\minikube\proxy-client.key
I0920 23:38:21.058046   34172 crypto.go:68] Generating cert C:\Users\mkuzm\.minikube\profiles\minikube\proxy-client.crt with IP's: []
I0920 23:38:21.614723   34172 crypto.go:156] Writing cert to C:\Users\mkuzm\.minikube\profiles\minikube\proxy-client.crt ...
I0920 23:38:21.615732   34172 lock.go:35] WriteFile acquiring C:\Users\mkuzm\.minikube\profiles\minikube\proxy-client.crt: {Name:mkd96014931707ebf5d197dd55f6492b8ec3b7f7 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0920 23:38:21.713268   34172 crypto.go:164] Writing key to C:\Users\mkuzm\.minikube\profiles\minikube\proxy-client.key ...
I0920 23:38:21.713268   34172 lock.go:35] WriteFile acquiring C:\Users\mkuzm\.minikube\profiles\minikube\proxy-client.key: {Name:mkeaaa175a60663f8052ca9947efe390dc8396b4 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0920 23:38:21.844575   34172 certs.go:484] found cert: C:\Users\mkuzm\.minikube\certs\ca-key.pem (1679 bytes)
I0920 23:38:21.849705   34172 certs.go:484] found cert: C:\Users\mkuzm\.minikube\certs\ca.pem (1074 bytes)
I0920 23:38:21.850592   34172 certs.go:484] found cert: C:\Users\mkuzm\.minikube\certs\cert.pem (1119 bytes)
I0920 23:38:21.851737   34172 certs.go:484] found cert: C:\Users\mkuzm\.minikube\certs\key.pem (1679 bytes)
I0920 23:38:22.059562   34172 ssh_runner.go:362] scp C:\Users\mkuzm\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0920 23:38:22.336784   34172 ssh_runner.go:362] scp C:\Users\mkuzm\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0920 23:38:22.491800   34172 ssh_runner.go:362] scp C:\Users\mkuzm\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0920 23:38:22.613771   34172 ssh_runner.go:362] scp C:\Users\mkuzm\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0920 23:38:22.721019   34172 ssh_runner.go:362] scp C:\Users\mkuzm\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0920 23:38:22.845937   34172 ssh_runner.go:362] scp C:\Users\mkuzm\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0920 23:38:22.936287   34172 ssh_runner.go:362] scp C:\Users\mkuzm\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0920 23:38:22.990232   34172 ssh_runner.go:362] scp C:\Users\mkuzm\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0920 23:38:23.059840   34172 ssh_runner.go:362] scp C:\Users\mkuzm\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0920 23:38:23.126448   34172 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (740 bytes)
I0920 23:38:23.196423   34172 ssh_runner.go:195] Run: openssl version
I0920 23:38:23.241126   34172 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0920 23:38:23.305481   34172 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0920 23:38:23.329142   34172 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Sep 20 21:38 /usr/share/ca-certificates/minikubeCA.pem
I0920 23:38:23.345182   34172 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0920 23:38:23.398582   34172 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0920 23:38:23.459673   34172 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0920 23:38:23.469162   34172 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0920 23:38:23.469775   34172 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\mkuzm:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0920 23:38:23.480565   34172 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0920 23:38:23.630741   34172 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0920 23:38:23.671648   34172 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0920 23:38:23.694145   34172 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I0920 23:38:23.716717   34172 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0920 23:38:23.745720   34172 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0920 23:38:23.745720   34172 kubeadm.go:157] found existing configuration files:

I0920 23:38:23.765708   34172 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0920 23:38:23.783912   34172 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0920 23:38:23.803073   34172 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0920 23:38:23.844193   34172 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0920 23:38:23.864136   34172 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0920 23:38:23.886538   34172 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0920 23:38:23.956536   34172 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0920 23:38:23.999847   34172 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0920 23:38:24.019125   34172 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0920 23:38:24.060377   34172 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0920 23:38:24.075957   34172 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0920 23:38:24.091702   34172 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0920 23:38:24.109103   34172 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.31.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0920 23:38:25.377223   34172 kubeadm.go:310] W0920 21:38:25.375150    1942 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "ClusterConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
I0920 23:38:25.380040   34172 kubeadm.go:310] W0920 21:38:25.378066    1942 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "InitConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
I0920 23:38:25.530231   34172 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I0920 23:38:26.003368   34172 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0920 23:40:54.404362   34172 kubeadm.go:310] [init] Using Kubernetes version: v1.31.0
I0920 23:40:54.408211   34172 kubeadm.go:310] [preflight] Running pre-flight checks
I0920 23:40:54.408756   34172 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0920 23:40:54.408756   34172 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0920 23:40:54.408949   34172 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0920 23:40:54.408949   34172 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0920 23:40:54.460855   34172 out.go:235]     ▪ Generating certificates and keys ...
I0920 23:40:54.469234   34172 kubeadm.go:310] [certs] Using existing ca certificate authority
I0920 23:40:54.470353   34172 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0920 23:40:54.472741   34172 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I0920 23:40:54.472741   34172 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I0920 23:40:54.472741   34172 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I0920 23:40:54.472741   34172 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I0920 23:40:54.473254   34172 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I0920 23:40:54.473254   34172 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0920 23:40:54.473692   34172 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I0920 23:40:54.475976   34172 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0920 23:40:54.475976   34172 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I0920 23:40:54.475976   34172 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I0920 23:40:54.476524   34172 kubeadm.go:310] [certs] Generating "sa" key and public key
I0920 23:40:54.476524   34172 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0920 23:40:54.476524   34172 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0920 23:40:54.477036   34172 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0920 23:40:54.477036   34172 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0920 23:40:54.477036   34172 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0920 23:40:54.477036   34172 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0920 23:40:54.477036   34172 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0920 23:40:54.477036   34172 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0920 23:40:54.483582   34172 out.go:235]     ▪ Booting up control plane ...
I0920 23:40:54.484748   34172 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0920 23:40:54.487351   34172 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0920 23:40:54.487351   34172 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0920 23:40:54.498113   34172 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0920 23:40:54.500220   34172 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0920 23:40:54.500818   34172 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0920 23:40:54.502409   34172 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0920 23:40:54.502600   34172 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0920 23:40:54.502600   34172 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 4.17852054s
I0920 23:40:54.502600   34172 kubeadm.go:310] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I0920 23:40:54.502600   34172 kubeadm.go:310] [api-check] The API server is healthy after 2m0.507707746s
I0920 23:40:54.502600   34172 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0920 23:40:54.503112   34172 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0920 23:40:54.503112   34172 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I0920 23:40:54.503112   34172 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0920 23:40:54.503112   34172 kubeadm.go:310] [bootstrap-token] Using token: v5txnr.x07yy83n6tk62byz
I0920 23:40:54.522108   34172 out.go:235]     ▪ Configuring RBAC rules ...
I0920 23:40:54.525138   34172 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0920 23:40:54.525138   34172 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0920 23:40:54.525733   34172 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0920 23:40:54.525733   34172 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0920 23:40:54.534510   34172 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0920 23:40:54.534510   34172 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0920 23:40:54.535051   34172 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0920 23:40:54.535051   34172 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I0920 23:40:54.535051   34172 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I0920 23:40:54.535051   34172 kubeadm.go:310] 
I0920 23:40:54.535051   34172 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I0920 23:40:54.535051   34172 kubeadm.go:310] 
I0920 23:40:54.535607   34172 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I0920 23:40:54.536593   34172 kubeadm.go:310] 
I0920 23:40:54.536593   34172 kubeadm.go:310]   mkdir -p $HOME/.kube
I0920 23:40:54.536593   34172 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0920 23:40:54.536593   34172 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0920 23:40:54.536593   34172 kubeadm.go:310] 
I0920 23:40:54.536593   34172 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I0920 23:40:54.536593   34172 kubeadm.go:310] 
I0920 23:40:54.537103   34172 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0920 23:40:54.537103   34172 kubeadm.go:310] 
I0920 23:40:54.537294   34172 kubeadm.go:310] You should now deploy a pod network to the cluster.
I0920 23:40:54.537294   34172 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0920 23:40:54.537803   34172 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0920 23:40:54.537803   34172 kubeadm.go:310] 
I0920 23:40:54.537944   34172 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I0920 23:40:54.537944   34172 kubeadm.go:310] and service account keys on each node and then running the following as root:
I0920 23:40:54.537944   34172 kubeadm.go:310] 
I0920 23:40:54.537944   34172 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token v5txnr.x07yy83n6tk62byz \
I0920 23:40:54.539130   34172 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:881ad71e413ef16a14b556fb2132ae7ff3ab51037155b26819236c7d3601af5d \
I0920 23:40:54.539130   34172 kubeadm.go:310] 	--control-plane 
I0920 23:40:54.539130   34172 kubeadm.go:310] 
I0920 23:40:54.539130   34172 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I0920 23:40:54.539130   34172 kubeadm.go:310] 
I0920 23:40:54.539554   34172 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token v5txnr.x07yy83n6tk62byz \
I0920 23:40:54.539554   34172 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:881ad71e413ef16a14b556fb2132ae7ff3ab51037155b26819236c7d3601af5d 
I0920 23:40:54.557148   34172 cni.go:84] Creating CNI manager for ""
I0920 23:40:54.560082   34172 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0920 23:40:54.580245   34172 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I0920 23:40:54.856791   34172 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0920 23:40:57.273193   34172 ssh_runner.go:235] Completed: sudo mkdir -p /etc/cni/net.d: (2.4164017s)
I0920 23:40:57.385596   34172 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0920 23:41:00.346277   34172 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0920 23:41:00.390589   34172 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.31.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0920 23:41:00.400562   34172 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2024_09_20T23_41_00_0700 minikube.k8s.io/version=v1.34.0 minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61 minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0920 23:41:00.768582   34172 ops.go:34] apiserver oom_adj: -16
I0920 23:41:14.876219   34172 ssh_runner.go:235] Completed: sudo /var/lib/minikube/binaries/v1.31.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig: (14.4850099s)
I0920 23:41:14.877840   34172 kubeadm.go:1113] duration metric: took 14.5321125s to wait for elevateKubeSystemPrivileges
I0920 23:41:15.071071   34172 ssh_runner.go:235] Completed: sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2024_09_20T23_41_00_0700 minikube.k8s.io/version=v1.34.0 minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61 minikube.k8s.io/name=minikube minikube.k8s.io/primary=true: (14.6700026s)
I0920 23:41:15.071722   34172 kubeadm.go:394] duration metric: took 2m51.6019473s to StartCluster
I0920 23:41:15.076358   34172 settings.go:142] acquiring lock: {Name:mkf1c4626fb246e830a9441624602c1b5cae241b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0920 23:41:15.086133   34172 settings.go:150] Updating kubeconfig:  C:\Users\mkuzm\.kube\config
I0920 23:41:15.187716   34172 lock.go:35] WriteFile acquiring C:\Users\mkuzm\.kube\config: {Name:mk0e95fd583e8dc65e1f5fbf4e969bc227aecee9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0920 23:41:15.191868   34172 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0920 23:41:15.195480   34172 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0920 23:41:15.199011   34172 out.go:177] 🔎  Verifying Kubernetes components...
I0920 23:41:15.196484   34172 addons.go:507] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0920 23:41:15.205243   34172 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0920 23:41:15.205243   34172 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0920 23:41:15.206244   34172 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0920 23:41:15.206244   34172 addons.go:234] Setting addon storage-provisioner=true in "minikube"
I0920 23:41:15.209777   34172 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I0920 23:41:15.217665   34172 host.go:66] Checking if "minikube" exists ...
I0920 23:41:15.242869   34172 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0920 23:41:15.297972   34172 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0920 23:41:15.300355   34172 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0920 23:41:15.632101   34172 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0920 23:41:15.675723   34172 addons.go:431] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0920 23:41:15.675723   34172 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0920 23:41:15.695992   34172 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0920 23:41:15.774182   34172 addons.go:234] Setting addon default-storageclass=true in "minikube"
I0920 23:41:15.774235   34172 host.go:66] Checking if "minikube" exists ...
I0920 23:41:15.793045   34172 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0920 23:41:16.031046   34172 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54829 SSHKeyPath:C:\Users\mkuzm\.minikube\machines\minikube\id_rsa Username:docker}
I0920 23:41:16.109183   34172 addons.go:431] installing /etc/kubernetes/addons/storageclass.yaml
I0920 23:41:16.109183   34172 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0920 23:41:16.119554   34172 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0920 23:41:16.381317   34172 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54829 SSHKeyPath:C:\Users\mkuzm\.minikube\machines\minikube\id_rsa Username:docker}
I0920 23:41:29.282761   34172 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0920 23:41:29.471586   34172 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0920 23:41:32.265629   34172 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (17.0737613s)
I0920 23:41:32.271825   34172 ssh_runner.go:235] Completed: sudo systemctl daemon-reload: (17.0241109s)
I0920 23:41:32.278866   34172 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.254 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0920 23:41:32.291675   34172 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0920 23:42:02.771959   34172 ssh_runner.go:235] Completed: sudo systemctl start kubelet: (30.4802836s)
I0920 23:42:02.774335   34172 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (33.4881511s)
I0920 23:42:02.774873   34172 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (33.2998354s)
I0920 23:42:02.775402   34172 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.254 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -": (30.4954687s)
I0920 23:42:02.778055   34172 start.go:971] {"host.minikube.internal": 192.168.65.254} host record injected into CoreDNS's ConfigMap
I0920 23:42:02.792872   34172 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0920 23:42:03.308080   34172 api_server.go:52] waiting for apiserver process to appear ...
I0920 23:42:03.346998   34172 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0920 23:42:03.403547   34172 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0920 23:42:03.491934   34172 addons.go:510] duration metric: took 48.2963684s for enable addons: enabled=[storage-provisioner default-storageclass]
I0920 23:42:03.857238   34172 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0920 23:42:04.085480   34172 api_server.go:72] duration metric: took 48.8889965s to wait for apiserver process to appear ...
I0920 23:42:04.086843   34172 api_server.go:88] waiting for apiserver healthz status ...
I0920 23:42:04.088333   34172 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:54833/healthz ...
I0920 23:42:04.263309   34172 api_server.go:279] https://127.0.0.1:54833/healthz returned 200:
ok
I0920 23:42:04.370880   34172 api_server.go:141] control plane version: v1.31.0
I0920 23:42:04.372754   34172 api_server.go:131] duration metric: took 285.3996ms to wait for apiserver health ...
I0920 23:42:04.377538   34172 system_pods.go:43] waiting for kube-system pods to appear ...
I0920 23:42:04.580511   34172 system_pods.go:59] 8 kube-system pods found
I0920 23:42:04.580511   34172 system_pods.go:61] "coredns-6f6b679f8f-7lkqd" [47808182-6955-46e3-96d4-b8800cf61646] Pending / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0920 23:42:04.580511   34172 system_pods.go:61] "coredns-6f6b679f8f-nbzxw" [7738001f-484d-4416-92aa-5409aa5d171a] Pending / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0920 23:42:04.580511   34172 system_pods.go:61] "etcd-minikube" [49539da0-0709-4bee-bfb8-f8d916a890f3] Running
I0920 23:42:04.580511   34172 system_pods.go:61] "kube-apiserver-minikube" [7d6f7802-256a-4070-9cad-745355fd4e78] Running
I0920 23:42:04.580511   34172 system_pods.go:61] "kube-controller-manager-minikube" [b27a9a42-0ca2-4c48-93fc-1bcfb07d0cc1] Running
I0920 23:42:04.580511   34172 system_pods.go:61] "kube-proxy-c724g" [e8b026d8-c009-4207-a4a8-fd1f20aa1aae] Pending / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0920 23:42:04.580511   34172 system_pods.go:61] "kube-scheduler-minikube" [878bc2e5-6ebd-42e9-93e3-d6b58886198b] Running
I0920 23:42:04.580511   34172 system_pods.go:61] "storage-provisioner" [4ed2ea7a-bab9-4672-be94-348a4fc9b02a] Pending / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0920 23:42:04.580511   34172 system_pods.go:74] duration metric: took 199.7456ms to wait for pod list to return data ...
I0920 23:42:04.581526   34172 kubeadm.go:582] duration metric: took 49.3840273s to wait for: map[apiserver:true system_pods:true]
I0920 23:42:04.582526   34172 node_conditions.go:102] verifying NodePressure condition ...
I0920 23:42:05.124896   34172 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0920 23:42:05.125449   34172 node_conditions.go:123] node cpu capacity is 8
I0920 23:42:05.130060   34172 node_conditions.go:105] duration metric: took 546.5354ms to run NodePressure ...
I0920 23:42:05.130060   34172 start.go:241] waiting for startup goroutines ...
I0920 23:42:05.130060   34172 start.go:246] waiting for cluster config update ...
I0920 23:42:05.130060   34172 start.go:255] writing updated cluster config ...
I0920 23:42:05.196900   34172 ssh_runner.go:195] Run: rm -f paused
I0920 23:42:09.580460   34172 start.go:600] kubectl: 1.30.0, cluster: 1.31.0 (minor skew: 1)
I0920 23:42:09.597419   34172 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Sep 20 21:50:38 minikube cri-dockerd[1622]: time="2024-09-20T21:50:37Z" level=error msg="Unable to get docker version: operation timeout: context deadline exceeded"
Sep 20 21:50:49 minikube dockerd[1353]: time="2024-09-20T21:50:49.190262953Z" level=error msg="Handler for GET /v1.43/version returned error: context canceled"
Sep 20 21:51:39 minikube dockerd[1353]: time="2024-09-20T21:51:39.356364288Z" level=info msg="ignoring event" container=781034f8b8b818d9acfcde56adf05a8f124a9aa1caff4cb0ca5d7298b6298a76 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 20 21:54:37 minikube cri-dockerd[1622]: time="2024-09-20T21:54:37Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/861d651d35573037bedbbe879a667e303894b26599d2f4b5520ce6b42884bd30/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 20 21:54:37 minikube cri-dockerd[1622]: time="2024-09-20T21:54:37Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d38798f060d71e51ec70c484fa73ebb9ccb782c9ce1a4359b6e3497de3062633/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 20 21:54:37 minikube cri-dockerd[1622]: time="2024-09-20T21:54:37Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/638053ef622c1bf31b94412ebca97c9ffd4d556a93d6c7cb046a1f1d19b9d335/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 20 21:54:37 minikube cri-dockerd[1622]: time="2024-09-20T21:54:37Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/974bcfd051481e2c0402d651a97a75b83b5d33395f412a96afed60689d651c95/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 20 21:55:15 minikube cri-dockerd[1622]: time="2024-09-20T21:55:15Z" level=info msg="Pulling image mongo:latest: dafa2b0c44d2: Downloading [===========>                                       ]   6.83MB/29.75MB"
Sep 20 21:55:25 minikube cri-dockerd[1622]: time="2024-09-20T21:55:25Z" level=info msg="Pulling image mongo:latest: dafa2b0c44d2: Downloading [===============================================>   ]  28.55MB/29.75MB"
Sep 20 21:55:35 minikube cri-dockerd[1622]: time="2024-09-20T21:55:35Z" level=info msg="Pulling image mongo:latest: 07ed7efc9402: Downloading [========>                                          ]  39.36MB/241.6MB"
Sep 20 21:55:45 minikube cri-dockerd[1622]: time="2024-09-20T21:55:45Z" level=info msg="Pulling image mongo:latest: 07ed7efc9402: Downloading [============>                                      ]  59.31MB/241.6MB"
Sep 20 21:55:55 minikube cri-dockerd[1622]: time="2024-09-20T21:55:55Z" level=info msg="Pulling image mongo:latest: 07ed7efc9402: Downloading [===============>                                   ]  75.47MB/241.6MB"
Sep 20 21:56:05 minikube cri-dockerd[1622]: time="2024-09-20T21:56:05Z" level=info msg="Pulling image mongo:latest: 07ed7efc9402: Downloading [=================>                                 ]  82.99MB/241.6MB"
Sep 20 21:56:15 minikube cri-dockerd[1622]: time="2024-09-20T21:56:15Z" level=info msg="Pulling image mongo:latest: 07ed7efc9402: Downloading [=====================>                             ]  106.1MB/241.6MB"
Sep 20 21:56:25 minikube cri-dockerd[1622]: time="2024-09-20T21:56:25Z" level=info msg="Pulling image mongo:latest: dafa2b0c44d2: Extracting [===============================>                   ]  18.68MB/29.75MB"
Sep 20 21:56:35 minikube cri-dockerd[1622]: time="2024-09-20T21:56:35Z" level=info msg="Pulling image mongo:latest: 07ed7efc9402: Downloading [==================================>                ]  166.5MB/241.6MB"
Sep 20 21:56:45 minikube cri-dockerd[1622]: time="2024-09-20T21:56:45Z" level=info msg="Pulling image mongo:latest: 07ed7efc9402: Downloading [=========================================>         ]  199.9MB/241.6MB"
Sep 20 21:56:55 minikube cri-dockerd[1622]: time="2024-09-20T21:56:55Z" level=info msg="Pulling image mongo:latest: 07ed7efc9402: Downloading [=================================================> ]  239.7MB/241.6MB"
Sep 20 21:57:05 minikube cri-dockerd[1622]: time="2024-09-20T21:57:05Z" level=info msg="Pulling image mongo:latest: 07ed7efc9402: Extracting [=>                                                 ]  8.913MB/241.6MB"
Sep 20 21:57:15 minikube cri-dockerd[1622]: time="2024-09-20T21:57:15Z" level=info msg="Pulling image mongo:latest: 07ed7efc9402: Extracting [=====>                                             ]  25.62MB/241.6MB"
Sep 20 21:57:25 minikube cri-dockerd[1622]: time="2024-09-20T21:57:25Z" level=info msg="Pulling image mongo:latest: 07ed7efc9402: Extracting [============>                                      ]  61.83MB/241.6MB"
Sep 20 21:57:35 minikube cri-dockerd[1622]: time="2024-09-20T21:57:35Z" level=info msg="Pulling image mongo:latest: 07ed7efc9402: Extracting [=================>                                 ]   86.9MB/241.6MB"
Sep 20 21:57:45 minikube cri-dockerd[1622]: time="2024-09-20T21:57:45Z" level=info msg="Pulling image mongo:latest: 07ed7efc9402: Extracting [=====================>                             ]  102.5MB/241.6MB"
Sep 20 21:57:55 minikube cri-dockerd[1622]: time="2024-09-20T21:57:55Z" level=info msg="Pulling image mongo:latest: 07ed7efc9402: Extracting [=====================>                             ]  103.6MB/241.6MB"
Sep 20 21:58:05 minikube cri-dockerd[1622]: time="2024-09-20T21:58:05Z" level=info msg="Pulling image mongo:latest: 07ed7efc9402: Extracting [=======================>                           ]  115.3MB/241.6MB"
Sep 20 21:58:15 minikube cri-dockerd[1622]: time="2024-09-20T21:58:15Z" level=info msg="Pulling image mongo:latest: 07ed7efc9402: Extracting [============================>                      ]  139.3MB/241.6MB"
Sep 20 21:58:25 minikube cri-dockerd[1622]: time="2024-09-20T21:58:25Z" level=info msg="Pulling image mongo:latest: 07ed7efc9402: Extracting [================================>                  ]  156.5MB/241.6MB"
Sep 20 21:58:35 minikube cri-dockerd[1622]: time="2024-09-20T21:58:35Z" level=info msg="Pulling image mongo:latest: 07ed7efc9402: Extracting [==================================>                ]  166.6MB/241.6MB"
Sep 20 21:58:45 minikube cri-dockerd[1622]: time="2024-09-20T21:58:45Z" level=info msg="Pulling image mongo:latest: 07ed7efc9402: Extracting [=========================================>         ]    200MB/241.6MB"
Sep 20 21:58:55 minikube cri-dockerd[1622]: time="2024-09-20T21:58:55Z" level=info msg="Pulling image mongo:latest: 07ed7efc9402: Extracting [===============================================>   ]  231.2MB/241.6MB"
Sep 20 21:59:05 minikube cri-dockerd[1622]: time="2024-09-20T21:59:05Z" level=info msg="Pulling image mongo:latest: 07ed7efc9402: Extracting [================================================>  ]  236.2MB/241.6MB"
Sep 20 21:59:14 minikube cri-dockerd[1622]: time="2024-09-20T21:59:14Z" level=info msg="Stop pulling image mongo:latest: Status: Downloaded newer image for mongo:latest"
Sep 20 21:59:18 minikube cri-dockerd[1622]: time="2024-09-20T21:59:18Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Sep 20 21:59:20 minikube cri-dockerd[1622]: time="2024-09-20T21:59:20Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Sep 20 21:59:32 minikube cri-dockerd[1622]: time="2024-09-20T21:59:32Z" level=info msg="Pulling image martinakuzmanovska/my-node-api:latest: 819bb913b5bd: Downloading [========================================>          ]   4.92MB/6.142MB"
Sep 20 21:59:42 minikube cri-dockerd[1622]: time="2024-09-20T21:59:42Z" level=info msg="Pulling image martinakuzmanovska/my-node-api:latest: 4ddba3401738: Downloading [================================================>  ]  47.27MB/48.5MB"
Sep 20 21:59:52 minikube cri-dockerd[1622]: time="2024-09-20T21:59:52Z" level=info msg="Pulling image martinakuzmanovska/my-node-api:latest: 4ddba3401738: Extracting [=========>                                         ]  8.847MB/48.5MB"
Sep 20 22:00:02 minikube cri-dockerd[1622]: time="2024-09-20T22:00:02Z" level=info msg="Pulling image martinakuzmanovska/my-node-api:latest: 4ddba3401738: Extracting [===============>                                   ]  14.75MB/48.5MB"
Sep 20 22:00:12 minikube cri-dockerd[1622]: time="2024-09-20T22:00:12Z" level=info msg="Pulling image martinakuzmanovska/my-node-api:latest: 4ddba3401738: Extracting [==================>                                ]  18.19MB/48.5MB"
Sep 20 22:00:22 minikube cri-dockerd[1622]: time="2024-09-20T22:00:22Z" level=info msg="Pulling image martinakuzmanovska/my-node-api:latest: 4ddba3401738: Extracting [==========================>                        ]  26.05MB/48.5MB"
Sep 20 22:00:32 minikube cri-dockerd[1622]: time="2024-09-20T22:00:32Z" level=info msg="Pulling image martinakuzmanovska/my-node-api:latest: 4ddba3401738: Extracting [=========================================>         ]   40.3MB/48.5MB"
Sep 20 22:00:42 minikube cri-dockerd[1622]: time="2024-09-20T22:00:42Z" level=info msg="Pulling image martinakuzmanovska/my-node-api:latest: 4ddba3401738: Extracting [==============================================>    ]  45.22MB/48.5MB"
Sep 20 22:00:52 minikube cri-dockerd[1622]: time="2024-09-20T22:00:52Z" level=info msg="Pulling image martinakuzmanovska/my-node-api:latest: 4ddba3401738: Extracting [================================================>  ]  46.69MB/48.5MB"
Sep 20 22:01:02 minikube cri-dockerd[1622]: time="2024-09-20T22:01:02Z" level=info msg="Pulling image martinakuzmanovska/my-node-api:latest: 4ddba3401738: Extracting [=================================================> ]  47.68MB/48.5MB"
Sep 20 22:01:13 minikube cri-dockerd[1622]: time="2024-09-20T22:01:13Z" level=info msg="Pulling image martinakuzmanovska/my-node-api:latest: 819bb913b5bd: Extracting [=================>                                 ]  2.097MB/6.142MB"
Sep 20 22:01:23 minikube cri-dockerd[1622]: time="2024-09-20T22:01:22Z" level=info msg="Pulling image martinakuzmanovska/my-node-api:latest: 819bb913b5bd: Extracting [================================>                  ]  3.932MB/6.142MB"
Sep 20 22:01:33 minikube cri-dockerd[1622]: time="2024-09-20T22:01:32Z" level=info msg="Pulling image martinakuzmanovska/my-node-api:latest: 819bb913b5bd: Extracting [========================================>          ]  4.981MB/6.142MB"
Sep 20 22:01:42 minikube cri-dockerd[1622]: time="2024-09-20T22:01:42Z" level=info msg="Pulling image martinakuzmanovska/my-node-api:latest: 819bb913b5bd: Extracting [===============================================>   ]  5.833MB/6.142MB"
Sep 20 22:01:49 minikube cri-dockerd[1622]: time="2024-09-20T22:01:49Z" level=info msg="Stop pulling image martinakuzmanovska/my-node-api:latest: Status: Downloaded newer image for martinakuzmanovska/my-node-api:latest"
Sep 20 22:02:07 minikube dockerd[1353]: time="2024-09-20T22:02:07.289358710Z" level=info msg="ignoring event" container=f9c6aa88cfc5a2abaf0cd0c0f69eb19b71ee364b3cd2c7b58101c2c935e95bd1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 20 22:02:10 minikube cri-dockerd[1622]: time="2024-09-20T22:02:10Z" level=info msg="Stop pulling image martinakuzmanovska/my-node-api:latest: Status: Image is up to date for martinakuzmanovska/my-node-api:latest"
Sep 20 22:02:11 minikube dockerd[1353]: time="2024-09-20T22:02:11.545755067Z" level=info msg="ignoring event" container=c9c8492a4ee875fc3b8051977f15e7b13673e3053d76f26830f2b8c0a6ebd8b9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 20 22:02:27 minikube cri-dockerd[1622]: time="2024-09-20T22:02:27Z" level=info msg="Stop pulling image martinakuzmanovska/my-node-api:latest: Status: Image is up to date for martinakuzmanovska/my-node-api:latest"
Sep 20 22:02:27 minikube dockerd[1353]: time="2024-09-20T22:02:27.866812903Z" level=info msg="ignoring event" container=0ddb78d83accfa119b9094bcbc499a10e3f6c757d6bf6ba708e063b1b4150166 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 20 22:02:54 minikube cri-dockerd[1622]: time="2024-09-20T22:02:54Z" level=info msg="Stop pulling image martinakuzmanovska/my-node-api:latest: Status: Image is up to date for martinakuzmanovska/my-node-api:latest"
Sep 20 22:02:54 minikube dockerd[1353]: time="2024-09-20T22:02:54.815258463Z" level=info msg="ignoring event" container=9e07dccaf18186fcaaca87983f97cbad644caf8a3e03545317db512edc74a81a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 20 22:03:45 minikube cri-dockerd[1622]: time="2024-09-20T22:03:45Z" level=info msg="Stop pulling image martinakuzmanovska/my-node-api:latest: Status: Image is up to date for martinakuzmanovska/my-node-api:latest"
Sep 20 22:03:46 minikube dockerd[1353]: time="2024-09-20T22:03:46.466841393Z" level=info msg="ignoring event" container=f22726f424b1be7e9d7f13f91f25b17616e1a22968735c5857ffba53cecc58ec module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 20 22:05:11 minikube cri-dockerd[1622]: time="2024-09-20T22:05:11Z" level=info msg="Stop pulling image martinakuzmanovska/my-node-api:latest: Status: Image is up to date for martinakuzmanovska/my-node-api:latest"
Sep 20 22:05:12 minikube dockerd[1353]: time="2024-09-20T22:05:12.557377425Z" level=info msg="ignoring event" container=4b38dbbba85fd0676a578ac2bacb0a932500e15a6eddfe50a4f7981859121088 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"


==> container status <==
CONTAINER           IMAGE                                                                                                    CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
4b38dbbba85fd       martinakuzmanovska/my-node-api@sha256:cc68ff5f4150157a4931f351ee3628397c2fa6c00dcf827ecc642e03715e06cf   18 seconds ago      Exited              nodeserver                5                   974bcfd051481       nodeapp-deployment-78c6d49c4b-jx98h
51df5b9c52ec7       mongo@sha256:a3d0dec40b95df592c7e7eef603019ba03b164998c3b394739e0dd4cd45d490d                            6 minutes ago       Running             mongo                     0                   d38798f060d71       mongo-deployment-5bfc646b54-72859
25025b0e4e4e5       mongo@sha256:a3d0dec40b95df592c7e7eef603019ba03b164998c3b394739e0dd4cd45d490d                            6 minutes ago       Running             mongo                     0                   638053ef622c1       mongo-deployment-5bfc646b54-kgl49
1efffe82a648e       mongo@sha256:a3d0dec40b95df592c7e7eef603019ba03b164998c3b394739e0dd4cd45d490d                            6 minutes ago       Running             mongo                     0                   861d651d35573       mongo-deployment-5bfc646b54-kh8jr
d7edc2b35d552       6e38f40d628db                                                                                            13 minutes ago      Running             storage-provisioner       2                   8d544d58690e9       storage-provisioner
781034f8b8b81       6e38f40d628db                                                                                            22 minutes ago      Exited              storage-provisioner       1                   8d544d58690e9       storage-provisioner
09e5111d78f64       cbb01a7bd410d                                                                                            23 minutes ago      Running             coredns                   0                   c7f98159df76a       coredns-6f6b679f8f-nbzxw
10e4cfb6235a1       ad83b2ca7b09e                                                                                            23 minutes ago      Running             kube-proxy                0                   a0c4326880f46       kube-proxy-c724g
33d16584c8ace       045733566833c                                                                                            24 minutes ago      Running             kube-controller-manager   1                   2335e9c71127a       kube-controller-manager-minikube
e505a0fea4911       1766f54c897f0                                                                                            26 minutes ago      Running             kube-scheduler            0                   d3af7c92869bf       kube-scheduler-minikube
34ffb6320281d       2e96e5913fc06                                                                                            26 minutes ago      Running             etcd                      0                   92fc1d3630879       etcd-minikube
43c1fa8fdfa09       604f5db92eaa8                                                                                            26 minutes ago      Running             kube-apiserver            0                   476292add1263       kube-apiserver-minikube


==> coredns [09e5111d78f6] <==
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.224543878s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.939008609s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.097123704s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.097753755s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.002089783s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.450801055s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.006070536s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.502824955s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 4.991694604s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.222183242s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.598580012s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.192202361s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.196981104s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.504164201s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.413819691s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.005350724s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.20004651s
[INFO] 10.244.0.6:45631 - 46170 "A IN compass.mongodb.com.default.svc.cluster.local. udp 63 false 512" NXDOMAIN qr,aa,rd 156 0.708206225s
[INFO] 10.244.0.6:45631 - 18503 "AAAA IN compass.mongodb.com.default.svc.cluster.local. udp 63 false 512" NXDOMAIN qr,aa,rd 156 0.701660219s
[INFO] 10.244.0.7:48437 - 21112 "AAAA IN compass.mongodb.com.default.svc.cluster.local. udp 63 false 512" NXDOMAIN qr,aa,rd 156 0.003248443s
[INFO] 10.244.0.7:48437 - 52859 "A IN compass.mongodb.com.default.svc.cluster.local. udp 63 false 512" NXDOMAIN qr,aa,rd 156 0.000201718s
[INFO] 10.244.0.7:37481 - 64844 "AAAA IN compass.mongodb.com.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.003299258s
[INFO] 10.244.0.6:54963 - 41671 "AAAA IN compass.mongodb.com.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000231887s
[INFO] 10.244.0.6:54963 - 25284 "A IN compass.mongodb.com.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.00253438s
[INFO] 10.244.0.7:37481 - 15441 "A IN compass.mongodb.com.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.07240158s
[INFO] 10.244.0.6:53803 - 43443 "AAAA IN compass.mongodb.com.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.002331198s
[INFO] 10.244.0.6:53803 - 60606 "A IN compass.mongodb.com.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000263278s
[INFO] 10.244.0.7:47153 - 23439 "A IN compass.mongodb.com.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.0008009s
[INFO] 10.244.0.7:47153 - 41088 "AAAA IN compass.mongodb.com.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.002352517s
[INFO] 10.244.0.6:36559 - 496 "A IN compass.mongodb.com. udp 37 false 512" NOERROR qr,rd,ra 306 0.415264723s
[INFO] 10.244.0.7:49267 - 8177 "A IN compass.mongodb.com. udp 37 false 512" NOERROR qr,rd,ra 306 0.322376489s
[INFO] 10.244.0.7:49267 - 48370 "AAAA IN compass.mongodb.com. udp 37 false 512" NOERROR qr,rd,ra 96 0.400601716s
[INFO] 10.244.0.6:36559 - 1533 "AAAA IN compass.mongodb.com. udp 37 false 512" NOERROR qr,rd,ra 96 0.492337107s
[INFO] 10.244.0.5:39567 - 11605 "AAAA IN compass.mongodb.com.default.svc.cluster.local. udp 63 false 512" NXDOMAIN qr,aa,rd 156 0.001400904s
[INFO] 10.244.0.5:39567 - 28497 "A IN compass.mongodb.com.default.svc.cluster.local. udp 63 false 512" NXDOMAIN qr,aa,rd 156 0.001408882s
[INFO] 10.244.0.5:41668 - 7235 "AAAA IN compass.mongodb.com.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.001145203s
[INFO] 10.244.0.5:41668 - 33358 "A IN compass.mongodb.com.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000214678s
[INFO] 10.244.0.5:58284 - 40539 "A IN compass.mongodb.com.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000178125s
[INFO] 10.244.0.5:58284 - 64599 "AAAA IN compass.mongodb.com.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.079548573s
[INFO] 10.244.0.5:42831 - 38115 "AAAA IN compass.mongodb.com. udp 37 false 512" NOERROR qr,aa,rd,ra 96 0.000392091s
[INFO] 10.244.0.5:42831 - 19424 "A IN compass.mongodb.com. udp 37 false 512" NOERROR qr,aa,rd,ra 306 0.000212423s
[INFO] 10.244.0.6:56442 - 1539 "A IN raw.githubusercontent.com.default.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.001378002s
[INFO] 10.244.0.6:56442 - 63243 "AAAA IN raw.githubusercontent.com.default.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.000661413s
[INFO] 10.244.0.6:45566 - 41455 "AAAA IN raw.githubusercontent.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.000248986s
[INFO] 10.244.0.6:45566 - 24301 "A IN raw.githubusercontent.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.001062083s
[INFO] 10.244.0.6:46633 - 8729 "AAAA IN raw.githubusercontent.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000210097s
[INFO] 10.244.0.6:46633 - 5382 "A IN raw.githubusercontent.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000562148s
[INFO] 10.244.0.6:42452 - 20561 "AAAA IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,rd,ra 255 0.095990661s
[INFO] 10.244.0.6:42452 - 13660 "A IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,rd,ra 207 0.196310011s
[INFO] 10.244.0.5:49437 - 41256 "A IN raw.githubusercontent.com.default.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.000282802s
[INFO] 10.244.0.5:49437 - 54315 "AAAA IN raw.githubusercontent.com.default.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.001038229s
[INFO] 10.244.0.5:36499 - 30250 "AAAA IN raw.githubusercontent.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.000215479s
[INFO] 10.244.0.5:36499 - 30503 "A IN raw.githubusercontent.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.000358246s
[INFO] 10.244.0.5:48382 - 26707 "A IN raw.githubusercontent.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.001009785s
[INFO] 10.244.0.5:48382 - 4432 "AAAA IN raw.githubusercontent.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000547283s
[INFO] 10.244.0.5:39175 - 27286 "AAAA IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,aa,rd,ra 255 0.00018475s
[INFO] 10.244.0.5:39175 - 53677 "A IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,aa,rd,ra 207 0.000295231s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_09_20T23_41_00_0700
                    minikube.k8s.io/version=v1.34.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 20 Sep 2024 21:40:18 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 20 Sep 2024 22:05:23 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 20 Sep 2024 22:02:18 +0000   Fri, 20 Sep 2024 21:51:49 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 20 Sep 2024 22:02:18 +0000   Fri, 20 Sep 2024 21:51:49 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 20 Sep 2024 22:02:18 +0000   Fri, 20 Sep 2024 21:51:49 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 20 Sep 2024 22:02:18 +0000   Fri, 20 Sep 2024 21:51:49 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3750732Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3750732Ki
  pods:               110
System Info:
  Machine ID:                 59f42c8e398d458a83fb2d761569e0f8
  System UUID:                59f42c8e398d458a83fb2d761569e0f8
  Boot ID:                    14ec08d2-a137-4680-b7a0-00ba252d2da4
  Kernel Version:             5.15.146.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.2.0
  Kubelet Version:            v1.31.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (11 in total)
  Namespace                   Name                                   CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                   ------------  ----------  ---------------  -------------  ---
  default                     mongo-deployment-5bfc646b54-72859      0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m
  default                     mongo-deployment-5bfc646b54-kgl49      0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m
  default                     mongo-deployment-5bfc646b54-kh8jr      0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m
  default                     nodeapp-deployment-78c6d49c4b-jx98h    0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m
  kube-system                 coredns-6f6b679f8f-nbzxw               100m (1%)     0 (0%)      70Mi (1%)        170Mi (4%)     24m
  kube-system                 etcd-minikube                          100m (1%)     0 (0%)      100Mi (2%)       0 (0%)         24m
  kube-system                 kube-apiserver-minikube                250m (3%)     0 (0%)      0 (0%)           0 (0%)         24m
  kube-system                 kube-controller-manager-minikube       200m (2%)     0 (0%)      0 (0%)           0 (0%)         25m
  kube-system                 kube-proxy-c724g                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         24m
  kube-system                 kube-scheduler-minikube                100m (1%)     0 (0%)      0 (0%)           0 (0%)         24m
  kube-system                 storage-provisioner                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         23m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (9%)   0 (0%)
  memory             170Mi (4%)  170Mi (4%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age                From             Message
  ----     ------                             ----               ----             -------
  Normal   Starting                           22m                kube-proxy       
  Normal   NodeHasSufficientMemory            26m (x7 over 26m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              26m (x7 over 26m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               26m (x7 over 26m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            26m                kubelet          Updated Node Allocatable limit across pods
  Normal   Starting                           24m                kubelet          Starting kubelet.
  Warning  PossibleMemoryBackedVolumesOnDisk  24m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Warning  CgroupV1                           24m                kubelet          Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.
  Normal   NodeAllocatableEnforced            24m                kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode                     24m                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   NodeNotReady                       13m                kubelet          Node minikube status is now: NodeNotReady
  Normal   NodeHasSufficientMemory            13m (x2 over 24m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              13m (x2 over 24m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               13m (x2 over 24m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeReady                          13m                kubelet          Node minikube status is now: NodeReady


==> dmesg <==
[Sep20 15:56] PCI: Fatal: No config space access function found
[  +0.021753] PCI: System does not support PCI
[  +0.023039] kvm: no hardware support
[  +9.418931] FS-Cache: Duplicate cookie detected
[  +0.002475] FS-Cache: O-cookie c=00000004 [p=00000002 fl=222 nc=0 na=1]
[  +0.000738] FS-Cache: O-cookie d=00000000afba8ddf{9P.session} n=0000000084bc6fa4
[  +0.002204] FS-Cache: O-key=[10] '34323934393338323435'
[  +0.037792] FS-Cache: N-cookie c=00000005 [p=00000002 fl=2 nc=0 na=1]
[  +0.002641] FS-Cache: N-cookie d=00000000afba8ddf{9P.session} n=000000001da23e4a
[  +0.000835] FS-Cache: N-key=[10] '34323934393338323435'
[  +1.657002] WSL (1) ERROR: ConfigApplyWindowsLibPath:2537: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000005]  failed 2
[  +0.031914] WSL (1) WARNING: /usr/share/zoneinfo/Europe/Skopje not found. Is the tzdata package installed?
[  +0.428350] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.004066] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001496] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.002311] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.150630] WSL (2) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.002566] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.010800] WSL (1) ERROR: ConfigMountFsTab:2589: Processing fstab with mount -a failed.
[  +0.003226] WSL (1) ERROR: ConfigApplyWindowsLibPath:2537: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000005]  failed 2
[  +0.004150] WSL (3) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.001574] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.119061] WSL (1) WARNING: /usr/share/zoneinfo/Europe/Skopje not found. Is the tzdata package installed?
[  +0.043657] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.011472] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.006915] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.012176] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +2.101338] netlink: 'init': attribute type 4 has an invalid length.
[  +0.774030] kmem.limit_in_bytes is deprecated and will be removed. Please report your usecase to linux-mm@kvack.org if you depend on this functionality.
[Sep20 20:09] WSL (1) WARNING: /usr/share/zoneinfo/Europe/Skopje not found. Is the tzdata package installed?
[  +0.029011] WSL (1) WARNING: /usr/share/zoneinfo/Europe/Skopje not found. Is the tzdata package installed?
[  +0.016919] WSL (1) WARNING: /usr/share/zoneinfo/Europe/Skopje not found. Is the tzdata package installed?
[  +0.014425] WSL (1) WARNING: /usr/share/zoneinfo/Europe/Skopje not found. Is the tzdata package installed?
[Sep20 21:38] tmpfs: Unknown parameter 'noswap'
[Sep20 21:41] tmpfs: Unknown parameter 'noswap'
[Sep20 21:47] hrtimer: interrupt took 217900887 ns
[  +0.002075] hrtimer: interrupt took 222845850 ns


==> etcd [34ffb6320281] <==
{"level":"warn","ts":"2024-09-20T22:01:57.976444Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"397.291572ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/horizontalpodautoscalers/\" range_end:\"/registry/horizontalpodautoscalers0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-09-20T22:01:57.976519Z","caller":"traceutil/trace.go:171","msg":"trace[67937945] range","detail":"{range_begin:/registry/horizontalpodautoscalers/; range_end:/registry/horizontalpodautoscalers0; response_count:0; response_revision:1272; }","duration":"397.378313ms","start":"2024-09-20T22:01:57.579119Z","end":"2024-09-20T22:01:57.976497Z","steps":["trace[67937945] 'agreement among raft nodes before linearized reading'  (duration: 83.122002ms)","trace[67937945] 'count revisions from in-memory index tree'  (duration: 314.105486ms)"],"step_count":2}
{"level":"warn","ts":"2024-09-20T22:01:57.976669Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-09-20T22:01:57.579055Z","time spent":"397.599799ms","remote":"127.0.0.1:39214","response type":"/etcdserverpb.KV/Range","request count":0,"request size":76,"response count":0,"response size":29,"request content":"key:\"/registry/horizontalpodautoscalers/\" range_end:\"/registry/horizontalpodautoscalers0\" count_only:true "}
{"level":"warn","ts":"2024-09-20T22:01:58.161877Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.48756742s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.49.2\" ","response":"range_response_count:1 size:131"}
{"level":"info","ts":"2024-09-20T22:01:58.161960Z","caller":"traceutil/trace.go:171","msg":"trace[730663915] range","detail":"{range_begin:/registry/masterleases/192.168.49.2; range_end:; response_count:1; response_revision:1272; }","duration":"1.487753174s","start":"2024-09-20T22:01:56.674184Z","end":"2024-09-20T22:01:58.161937Z","steps":["trace[730663915] 'agreement among raft nodes before linearized reading'  (duration: 907.104655ms)","trace[730663915] 'range keys from in-memory index tree'  (duration: 380.020289ms)","trace[730663915] 'assemble the response'  (duration: 200.386062ms)"],"step_count":3}
{"level":"warn","ts":"2024-09-20T22:01:58.162001Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-09-20T22:01:56.674093Z","time spent":"1.487893824s","remote":"127.0.0.1:39014","response type":"/etcdserverpb.KV/Range","request count":0,"request size":37,"response count":1,"response size":155,"request content":"key:\"/registry/masterleases/192.168.49.2\" "}
{"level":"warn","ts":"2024-09-20T22:01:58.177788Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"596.573784ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-09-20T22:01:58.177880Z","caller":"traceutil/trace.go:171","msg":"trace[1271632872] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1272; }","duration":"596.692768ms","start":"2024-09-20T22:01:57.581166Z","end":"2024-09-20T22:01:58.177859Z","steps":["trace[1271632872] 'agreement among raft nodes before linearized reading'  (duration: 81.153584ms)","trace[1271632872] 'range keys from in-memory index tree'  (duration: 314.784059ms)","trace[1271632872] 'filter and sort the key-value pairs'  (duration: 190.3505ms)"],"step_count":3}
{"level":"warn","ts":"2024-09-20T22:01:58.177928Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-09-20T22:01:57.581110Z","time spent":"596.810799ms","remote":"127.0.0.1:39004","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-09-20T22:01:58.967722Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"198.763915ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-09-20T22:01:58.967842Z","caller":"traceutil/trace.go:171","msg":"trace[1979210347] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1272; }","duration":"205.232346ms","start":"2024-09-20T22:01:58.762583Z","end":"2024-09-20T22:01:58.967815Z","steps":["trace[1979210347] 'range keys from in-memory index tree'  (duration: 198.741197ms)"],"step_count":1}
{"level":"warn","ts":"2024-09-20T22:01:58.980219Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"411.00452ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-09-20T22:01:58.980354Z","caller":"traceutil/trace.go:171","msg":"trace[1281509623] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1272; }","duration":"411.161692ms","start":"2024-09-20T22:01:58.569174Z","end":"2024-09-20T22:01:58.980335Z","steps":["trace[1281509623] 'range keys from in-memory index tree'  (duration: 410.903753ms)"],"step_count":1}
{"level":"warn","ts":"2024-09-20T22:01:58.980909Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-09-20T22:01:58.569104Z","time spent":"411.397495ms","remote":"127.0.0.1:38998","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-09-20T22:02:00.356533Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"189.030398ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128032030820962095 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:1264 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128032030820962092 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:16"}
{"level":"warn","ts":"2024-09-20T22:02:00.362632Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.309837873s","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-09-20T22:02:00.363677Z","caller":"traceutil/trace.go:171","msg":"trace[1656441098] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1273; }","duration":"1.309929724s","start":"2024-09-20T22:01:59.061752Z","end":"2024-09-20T22:02:00.362685Z","steps":["trace[1656441098] 'agreement among raft nodes before linearized reading'  (duration: 1.309798007s)"],"step_count":1}
{"level":"info","ts":"2024-09-20T22:02:00.363746Z","caller":"traceutil/trace.go:171","msg":"trace[337742769] transaction","detail":"{read_only:false; response_revision:1273; number_of_response:1; }","duration":"1.5090877s","start":"2024-09-20T22:01:58.863089Z","end":"2024-09-20T22:02:00.363181Z","steps":["trace[337742769] 'process raft request'  (duration: 24.476488ms)","trace[337742769] 'compare'  (duration: 97.435987ms)","trace[337742769] 'store kv pair into bolt db' {req_type:put; key:/registry/masterleases/192.168.49.2; req_size:114; } (duration: 91.437691ms)"],"step_count":3}
{"level":"warn","ts":"2024-09-20T22:02:00.363999Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-09-20T22:01:58.863043Z","time spent":"1.509786211s","remote":"127.0.0.1:39014","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":116,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:1264 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128032030820962092 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >"}
{"level":"info","ts":"2024-09-20T22:02:00.360603Z","caller":"traceutil/trace.go:171","msg":"trace[1503869091] linearizableReadLoop","detail":"{readStateIndex:1529; appliedIndex:1528; }","duration":"1.307761505s","start":"2024-09-20T22:01:59.061773Z","end":"2024-09-20T22:02:00.360538Z","steps":["trace[1503869091] 'read index received'  (duration: 53.687µs)","trace[1503869091] 'applied index is now lower than readState.Index'  (duration: 1.307704683s)"],"step_count":2}
{"level":"warn","ts":"2024-09-20T22:02:00.459114Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"100.723699ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128032030820962096 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:1266 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >>","response":"size:16"}
{"level":"info","ts":"2024-09-20T22:02:00.460378Z","caller":"traceutil/trace.go:171","msg":"trace[715708377] transaction","detail":"{read_only:false; response_revision:1274; number_of_response:1; }","duration":"807.679073ms","start":"2024-09-20T22:01:59.652660Z","end":"2024-09-20T22:02:00.460339Z","steps":["trace[715708377] 'process raft request'  (duration: 703.949651ms)"],"step_count":1}
{"level":"warn","ts":"2024-09-20T22:02:00.461207Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-09-20T22:01:59.652632Z","time spent":"807.832818ms","remote":"127.0.0.1:39244","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":673,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:1266 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"warn","ts":"2024-09-20T22:02:00.678102Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"209.711582ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" ","response":"range_response_count:1 size:420"}
{"level":"warn","ts":"2024-09-20T22:02:00.752539Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"279.131769ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2024-09-20T22:02:00.752610Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"288.600466ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/validatingadmissionpolicybindings/\" range_end:\"/registry/validatingadmissionpolicybindings0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-09-20T22:02:00.752720Z","caller":"traceutil/trace.go:171","msg":"trace[933356661] range","detail":"{range_begin:/registry/validatingadmissionpolicybindings/; range_end:/registry/validatingadmissionpolicybindings0; response_count:0; response_revision:1274; }","duration":"288.712599ms","start":"2024-09-20T22:02:00.463993Z","end":"2024-09-20T22:02:00.752706Z","steps":["trace[933356661] 'count revisions from in-memory index tree'  (duration: 285.077523ms)"],"step_count":1}
{"level":"info","ts":"2024-09-20T22:02:00.678322Z","caller":"traceutil/trace.go:171","msg":"trace[497022908] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:1274; }","duration":"211.104479ms","start":"2024-09-20T22:02:00.467191Z","end":"2024-09-20T22:02:00.678295Z","steps":["trace[497022908] 'range keys from in-memory index tree'  (duration: 209.371416ms)"],"step_count":1}
{"level":"info","ts":"2024-09-20T22:02:00.752602Z","caller":"traceutil/trace.go:171","msg":"trace[83569186] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1274; }","duration":"279.204103ms","start":"2024-09-20T22:02:00.473380Z","end":"2024-09-20T22:02:00.752584Z","steps":["trace[83569186] 'range keys from in-memory index tree'  (duration: 279.107307ms)"],"step_count":1}
{"level":"info","ts":"2024-09-20T22:02:00.879272Z","caller":"traceutil/trace.go:171","msg":"trace[870032788] transaction","detail":"{read_only:false; response_revision:1275; number_of_response:1; }","duration":"103.564318ms","start":"2024-09-20T22:02:00.774813Z","end":"2024-09-20T22:02:00.878377Z","steps":["trace[870032788] 'process raft request'  (duration: 85.062385ms)"],"step_count":1}
{"level":"warn","ts":"2024-09-20T22:02:02.368079Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"594.955455ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128032030820962108 > lease_revoke:<id:70cc921160148ef6>","response":"size:29"}
{"level":"info","ts":"2024-09-20T22:02:02.368261Z","caller":"traceutil/trace.go:171","msg":"trace[1989181319] linearizableReadLoop","detail":"{readStateIndex:1532; appliedIndex:1531; }","duration":"290.618998ms","start":"2024-09-20T22:02:02.077622Z","end":"2024-09-20T22:02:02.368241Z","steps":["trace[1989181319] 'read index received'  (duration: 55.826µs)","trace[1989181319] 'applied index is now lower than readState.Index'  (duration: 290.560848ms)"],"step_count":2}
{"level":"warn","ts":"2024-09-20T22:02:02.368578Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"290.933869ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/daemonsets/\" range_end:\"/registry/daemonsets0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-09-20T22:02:02.369194Z","caller":"traceutil/trace.go:171","msg":"trace[2092386289] range","detail":"{range_begin:/registry/daemonsets/; range_end:/registry/daemonsets0; response_count:0; response_revision:1275; }","duration":"290.985319ms","start":"2024-09-20T22:02:02.077612Z","end":"2024-09-20T22:02:02.368597Z","steps":["trace[2092386289] 'agreement among raft nodes before linearized reading'  (duration: 290.902646ms)"],"step_count":1}
{"level":"warn","ts":"2024-09-20T22:02:02.371557Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"117.737121ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-09-20T22:02:02.371619Z","caller":"traceutil/trace.go:171","msg":"trace[1860855291] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1275; }","duration":"117.817509ms","start":"2024-09-20T22:02:02.253787Z","end":"2024-09-20T22:02:02.371605Z","steps":["trace[1860855291] 'agreement among raft nodes before linearized reading'  (duration: 117.691713ms)"],"step_count":1}
{"level":"info","ts":"2024-09-20T22:02:02.468792Z","caller":"traceutil/trace.go:171","msg":"trace[1974342232] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1275; }","duration":"396.319324ms","start":"2024-09-20T22:02:02.072444Z","end":"2024-09-20T22:02:02.468763Z","steps":["trace[1974342232] 'get authentication metadata'  (duration: 396.290154ms)"],"step_count":1}
{"level":"warn","ts":"2024-09-20T22:02:14.358045Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"305.752537ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128032030820962177 > lease_revoke:<id:70cc921160148f2c>","response":"size:29"}
{"level":"info","ts":"2024-09-20T22:02:14.358684Z","caller":"traceutil/trace.go:171","msg":"trace[1516498524] linearizableReadLoop","detail":"{readStateIndex:1551; appliedIndex:1550; }","duration":"304.582741ms","start":"2024-09-20T22:02:14.054042Z","end":"2024-09-20T22:02:14.358625Z","steps":["trace[1516498524] 'read index received'  (duration: 38.227µs)","trace[1516498524] 'applied index is now lower than readState.Index'  (duration: 304.541434ms)"],"step_count":2}
{"level":"warn","ts":"2024-09-20T22:02:14.359079Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"304.988056ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-09-20T22:02:14.359204Z","caller":"traceutil/trace.go:171","msg":"trace[1001743247] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1292; }","duration":"305.079234ms","start":"2024-09-20T22:02:14.054029Z","end":"2024-09-20T22:02:14.359109Z","steps":["trace[1001743247] 'agreement among raft nodes before linearized reading'  (duration: 304.95662ms)"],"step_count":1}
{"level":"warn","ts":"2024-09-20T22:02:14.359355Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-09-20T22:02:14.053973Z","time spent":"305.320108ms","remote":"127.0.0.1:39004","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-09-20T22:02:51.549087Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"180.946078ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-09-20T22:02:51.648830Z","caller":"traceutil/trace.go:171","msg":"trace[832040095] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1333; }","duration":"280.783897ms","start":"2024-09-20T22:02:51.368012Z","end":"2024-09-20T22:02:51.648796Z","steps":["trace[832040095] 'range keys from in-memory index tree'  (duration: 180.920089ms)"],"step_count":1}
{"level":"warn","ts":"2024-09-20T22:02:52.052332Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"303.176699ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128032030820962394 > lease_revoke:<id:70cc921160149008>","response":"size:29"}
{"level":"info","ts":"2024-09-20T22:02:52.054202Z","caller":"traceutil/trace.go:171","msg":"trace[1745623733] linearizableReadLoop","detail":"{readStateIndex:1601; appliedIndex:1600; }","duration":"404.859486ms","start":"2024-09-20T22:02:51.649312Z","end":"2024-09-20T22:02:52.054171Z","steps":["trace[1745623733] 'read index received'  (duration: 99.728166ms)","trace[1745623733] 'applied index is now lower than readState.Index'  (duration: 305.127121ms)"],"step_count":2}
{"level":"warn","ts":"2024-09-20T22:02:52.054350Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"405.018716ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-09-20T22:02:52.054377Z","caller":"traceutil/trace.go:171","msg":"trace[1307245085] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1333; }","duration":"405.117671ms","start":"2024-09-20T22:02:51.649251Z","end":"2024-09-20T22:02:52.054368Z","steps":["trace[1307245085] 'agreement among raft nodes before linearized reading'  (duration: 405.038035ms)"],"step_count":1}
{"level":"info","ts":"2024-09-20T22:03:41.453876Z","caller":"traceutil/trace.go:171","msg":"trace[323508701] transaction","detail":"{read_only:false; response_revision:1382; number_of_response:1; }","duration":"192.488188ms","start":"2024-09-20T22:03:41.261361Z","end":"2024-09-20T22:03:41.453850Z","steps":["trace[323508701] 'process raft request'  (duration: 192.162953ms)"],"step_count":1}
{"level":"warn","ts":"2024-09-20T22:04:11.475773Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"105.346574ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/\" range_end:\"/registry/serviceaccounts0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-09-20T22:04:11.477890Z","caller":"traceutil/trace.go:171","msg":"trace[566183771] range","detail":"{range_begin:/registry/serviceaccounts/; range_end:/registry/serviceaccounts0; response_count:0; response_revision:1409; }","duration":"127.494466ms","start":"2024-09-20T22:04:11.349893Z","end":"2024-09-20T22:04:11.477388Z","steps":["trace[566183771] 'count revisions from in-memory index tree'  (duration: 104.763426ms)"],"step_count":1}
{"level":"info","ts":"2024-09-20T22:04:30.409974Z","caller":"traceutil/trace.go:171","msg":"trace[797849521] transaction","detail":"{read_only:false; response_revision:1425; number_of_response:1; }","duration":"181.248352ms","start":"2024-09-20T22:04:30.228616Z","end":"2024-09-20T22:04:30.409864Z","steps":["trace[797849521] 'process raft request'  (duration: 181.129463ms)"],"step_count":1}
{"level":"info","ts":"2024-09-20T22:04:30.501439Z","caller":"traceutil/trace.go:171","msg":"trace[1839611813] linearizableReadLoop","detail":"{readStateIndex:1713; appliedIndex:1712; }","duration":"143.271548ms","start":"2024-09-20T22:04:30.357984Z","end":"2024-09-20T22:04:30.501256Z","steps":["trace[1839611813] 'read index received'  (duration: 51.652254ms)","trace[1839611813] 'applied index is now lower than readState.Index'  (duration: 91.615497ms)"],"step_count":2}
{"level":"warn","ts":"2024-09-20T22:04:30.520202Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"162.143388ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2024-09-20T22:04:30.520217Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"148.238337ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/rolebindings/\" range_end:\"/registry/rolebindings0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-09-20T22:04:30.520405Z","caller":"traceutil/trace.go:171","msg":"trace[1132174853] range","detail":"{range_begin:/registry/rolebindings/; range_end:/registry/rolebindings0; response_count:0; response_revision:1425; }","duration":"148.492873ms","start":"2024-09-20T22:04:30.371865Z","end":"2024-09-20T22:04:30.520358Z","steps":["trace[1132174853] 'agreement among raft nodes before linearized reading'  (duration: 129.816281ms)","trace[1132174853] 'count revisions from in-memory index tree'  (duration: 18.319929ms)"],"step_count":2}
{"level":"info","ts":"2024-09-20T22:04:30.520361Z","caller":"traceutil/trace.go:171","msg":"trace[748939127] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1425; }","duration":"162.314613ms","start":"2024-09-20T22:04:30.357978Z","end":"2024-09-20T22:04:30.520292Z","steps":["trace[748939127] 'agreement among raft nodes before linearized reading'  (duration: 143.497102ms)","trace[748939127] 'range keys from in-memory index tree'  (duration: 18.567551ms)"],"step_count":2}
{"level":"info","ts":"2024-09-20T22:05:02.473269Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1194}
{"level":"info","ts":"2024-09-20T22:05:02.561618Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":1194,"took":"81.313244ms","hash":2632656833,"current-db-size-bytes":2068480,"current-db-size":"2.1 MB","current-db-size-in-use-bytes":1773568,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2024-09-20T22:05:02.562386Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2632656833,"revision":1194,"compact-revision":966}


==> kernel <==
 22:05:30 up  6:08,  0 users,  load average: 0.96, 4.73, 8.39
Linux minikube 5.15.146.1-microsoft-standard-WSL2 #1 SMP Thu Jan 11 04:09:03 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [43c1fa8fdfa0] <==
I0920 21:40:39.110202       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0920 21:40:40.148456       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0920 21:40:42.080176       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0920 21:40:42.285288       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0920 21:40:42.448113       1 controller.go:615] quota admission added evaluator for: endpoints
I0920 21:40:42.659494       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0920 21:40:51.783809       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0920 21:40:52.141011       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0920 21:40:52.401289       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0920 21:40:53.392789       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0920 21:41:16.847602       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0920 21:41:17.048790       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
E0920 21:47:42.079988       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 2.624712ms, panicked: false, err: <nil>, panic-reason: <nil>" logger="UnhandledError"
E0920 21:47:42.298267       1 writers.go:122] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0920 21:47:42.401656       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0920 21:47:50.183475       1 writers.go:135] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0920 21:48:00.103928       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="15.501709553s" method="PUT" path="/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath" result=null
W0920 21:48:21.614432       1 logging.go:55] [core] [Channel #223 SubChannel #224]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
E0920 21:49:22.819421       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 4.045375807s, panicked: false, err: context deadline exceeded, panic-reason: <nil>" logger="UnhandledError"
W0920 21:49:34.656922       1 logging.go:55] [core] [Channel #226 SubChannel #227]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
E0920 21:49:37.244205       1 controller.go:195] "Failed to update lease" err="Timeout: request did not complete within requested timeout - context deadline exceeded"
E0920 21:49:37.438690       1 controller.go:163] "Unhandled Error" err="unable to sync kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service" logger="UnhandledError"
E0920 21:49:37.564075       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 11.725759178s, panicked: false, err: context deadline exceeded, panic-reason: <nil>" logger="UnhandledError"
E0920 21:49:40.363486       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 31.915363794s, panicked: false, err: context deadline exceeded, panic-reason: <nil>" logger="UnhandledError"
E0920 21:49:59.054553       1 compact.go:124] etcd: endpoint ([https://127.0.0.1:2379]) compact failed: etcdserver: request timed out
E0920 21:50:06.277787       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
E0920 21:50:06.458216       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
E0920 21:50:06.469628       1 controller.go:195] "Failed to update lease" err="etcdserver: request timed out"
E0920 21:50:09.072178       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
E0920 21:50:08.973981       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
E0920 21:50:09.276067       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
E0920 21:50:23.063696       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
E0920 21:50:23.555591       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
E0920 21:50:24.660528       1 controller.go:195] "Failed to update lease" err="etcdserver: request timed out"
E0920 21:50:33.681670       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
E0920 21:50:34.090131       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
E0920 21:50:34.384298       1 controller.go:195] "Failed to update lease" err="etcdserver: request timed out"
E0920 21:50:34.792475       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
E0920 21:50:34.797996       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
E0920 21:50:35.091158       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
E0920 21:50:35.492536       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
E0920 21:50:35.608553       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
E0920 21:50:39.487708       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": the object has been modified; please apply your changes to the latest version and try again"
W0920 21:50:41.090412       1 aggregator.go:166] failed to download k8s_internal_local_delegation_chain_0000000001: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: request timed out, Header: map[]
E0920 21:50:55.113924       1 writers.go:122] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0920 21:50:55.273393       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0920 21:50:55.380087       1 writers.go:135] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0920 21:50:55.580223       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 1.19905ms, panicked: false, err: context canceled, panic-reason: <nil>" logger="UnhandledError"
E0920 21:50:55.480648       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="476.290049ms" method="PUT" path="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube" result=null
E0920 21:51:03.448576       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
E0920 21:51:03.538238       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
E0920 21:51:03.628642       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
E0920 21:51:03.732958       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
E0920 21:51:03.739606       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
E0920 21:51:03.745802       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
E0920 21:51:03.837138       1 controller.go:163] "Unhandled Error" err="unable to sync kubernetes service: etcdserver: request timed out" logger="UnhandledError"
E0920 21:51:07.029967       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="1m3.297961044s" method="GET" path="/openapi/v2" result=null
E0920 21:51:24.737221       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
I0920 21:53:35.819371       1 alloc.go:330] "allocated clusterIPs" service="default/mongo-service" clusterIPs={"IPv4":"10.111.53.117"}
I0920 21:53:59.225963       1 alloc.go:330] "allocated clusterIPs" service="default/nodeapp-service" clusterIPs={"IPv4":"10.102.34.54"}


==> kube-controller-manager [33d16584c8ac] <==
E0920 21:50:06.670276       1 event.go:359] "Server rejected event (will not retry!)" err="etcdserver: request timed out" logger="node-lifecycle-controller" event="&Event{ObjectMeta:{coredns-6f6b679f8f-nbzxw.17f712310adee2fa  kube-system    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Pod,Namespace:kube-system,Name:coredns-6f6b679f8f-nbzxw,UID:7738001f-484d-4416-92aa-5409aa5d171a,APIVersion:v1,ResourceVersion:494,FieldPath:,},Reason:NodeNotReady,Message:Node is not ready,Source:EventSource{Component:node-controller,Host:,},FirstTimestamp:2024-09-20 21:49:43.96495129 +0000 UTC m=+538.124145861,LastTimestamp:2024-09-20 21:49:43.96495129 +0000 UTC m=+538.124145861,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:node-controller,ReportingInstance:,}"
I0920 21:50:10.283482       1 event.go:377] Event(v1.ObjectReference{Kind:"Service", Namespace:"kube-system", Name:"kube-dns", UID:"f7061c97-bc19-43b6-8ede-6eb31cdeef49", APIVersion:"v1", ResourceVersion:"294", FieldPath:""}): type: 'Warning' reason: 'FailedToUpdateEndpointSlices' Error updating Endpoint Slices for Service kube-system/kube-dns: failed to update kube-dns-vn94r EndpointSlice for Service kube-system/kube-dns: etcdserver: request timed out
I0920 21:50:10.355147       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="kube-system/kube-dns" err="failed to update kube-dns-vn94r EndpointSlice for Service kube-system/kube-dns: etcdserver: request timed out"
I0920 21:50:10.669459       1 controller_utils.go:151] "Failed to update status for pod" logger="node-lifecycle-controller" pod="kube-system/kube-proxy-c724g" err="etcdserver: request timed out"
I0920 21:50:34.381859       1 event.go:377] Event(v1.ObjectReference{Kind:"Service", Namespace:"kube-system", Name:"kube-dns", UID:"f7061c97-bc19-43b6-8ede-6eb31cdeef49", APIVersion:"v1", ResourceVersion:"294", FieldPath:""}): type: 'Warning' reason: 'FailedToUpdateEndpointSlices' Error updating Endpoint Slices for Service kube-system/kube-dns: failed to update kube-dns-vn94r EndpointSlice for Service kube-system/kube-dns: etcdserver: request timed out
I0920 21:50:34.384175       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="kube-system/kube-dns" err="failed to update kube-dns-vn94r EndpointSlice for Service kube-system/kube-dns: etcdserver: request timed out"
E0920 21:50:34.907648       1 event.go:359] "Server rejected event (will not retry!)" err="etcdserver: request timed out" logger="endpoints-controller" event="&Event{ObjectMeta:{kube-dns.17f71237366d70a8  kube-system    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Endpoints,Namespace:kube-system,Name:kube-dns,UID:09320009-1b08-4ecb-8fe1-9580260818be,APIVersion:v1,ResourceVersion:495,FieldPath:,},Reason:FailedToUpdateEndpoint,Message:Failed to update endpoint kube-system/kube-dns: etcdserver: request timed out,Source:EventSource{Component:endpoint-controller,Host:,},FirstTimestamp:2024-09-20 21:50:10.465517736 +0000 UTC m=+564.606924250,LastTimestamp:2024-09-20 21:50:10.465517736 +0000 UTC m=+564.606924250,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:endpoint-controller,ReportingInstance:,}"
E0920 21:50:35.209797       1 event.go:359] "Server rejected event (will not retry!)" err="etcdserver: request timed out" logger="endpointslice-controller" event="&Event{ObjectMeta:{kube-dns.17f7123724c0e6a0  kube-system    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Service,Namespace:kube-system,Name:kube-dns,UID:f7061c97-bc19-43b6-8ede-6eb31cdeef49,APIVersion:v1,ResourceVersion:294,FieldPath:,},Reason:FailedToUpdateEndpointSlices,Message:Error updating Endpoint Slices for Service kube-system/kube-dns: failed to update kube-dns-vn94r EndpointSlice for Service kube-system/kube-dns: etcdserver: request timed out,Source:EventSource{Component:endpoint-slice-controller,Host:,},FirstTimestamp:2024-09-20 21:50:10.168997536 +0000 UTC m=+564.310404090,LastTimestamp:2024-09-20 21:50:10.168997536 +0000 UTC m=+564.310404090,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:endpoint-slice-controller,ReportingInstance:,}"
E0920 21:50:35.887739       1 event.go:359] "Server rejected event (will not retry!)" err="etcdserver: request timed out" logger="node-lifecycle-controller" event="&Event{ObjectMeta:{kube-proxy-c724g.17f7123742aebc53  kube-system    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Pod,Namespace:kube-system,Name:kube-proxy-c724g,UID:e8b026d8-c009-4207-a4a8-fd1f20aa1aae,APIVersion:v1,ResourceVersion:458,FieldPath:,},Reason:NodeNotReady,Message:Node is not ready,Source:EventSource{Component:node-controller,Host:,},FirstTimestamp:2024-09-20 21:50:10.671123539 +0000 UTC m=+564.812530093,LastTimestamp:2024-09-20 21:50:10.671123539 +0000 UTC m=+564.812530093,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:node-controller,ReportingInstance:,}"
I0920 21:50:36.488569       1 controller_utils.go:151] "Failed to update status for pod" logger="node-lifecycle-controller" pod="kube-system/storage-provisioner" err="etcdserver: request timed out"
I0920 21:51:03.928323       1 controller_utils.go:151] "Failed to update status for pod" logger="node-lifecycle-controller" pod="kube-system/etcd-minikube" err="etcdserver: request timed out"
I0920 21:51:03.924890       1 event.go:377] Event(v1.ObjectReference{Kind:"Service", Namespace:"kube-system", Name:"kube-dns", UID:"f7061c97-bc19-43b6-8ede-6eb31cdeef49", APIVersion:"v1", ResourceVersion:"294", FieldPath:""}): type: 'Warning' reason: 'FailedToUpdateEndpointSlices' Error updating Endpoint Slices for Service kube-system/kube-dns: failed to update kube-dns-vn94r EndpointSlice for Service kube-system/kube-dns: Operation cannot be fulfilled on endpointslices.discovery.k8s.io "kube-dns-vn94r": the object has been modified; please apply your changes to the latest version and try again
I0920 21:51:04.022454       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="kube-system/kube-dns" err="failed to update kube-dns-vn94r EndpointSlice for Service kube-system/kube-dns: Operation cannot be fulfilled on endpointslices.discovery.k8s.io \"kube-dns-vn94r\": the object has been modified; please apply your changes to the latest version and try again"
E0920 21:51:04.036621       1 event.go:359] "Server rejected event (will not retry!)" err="etcdserver: request timed out" logger="endpoints-controller" event="&Event{ObjectMeta:{kube-dns.17f71237366d70a8  kube-system    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Endpoints,Namespace:kube-system,Name:kube-dns,UID:09320009-1b08-4ecb-8fe1-9580260818be,APIVersion:v1,ResourceVersion:495,FieldPath:,},Reason:FailedToUpdateEndpoint,Message:Failed to update endpoint kube-system/kube-dns: etcdserver: request timed out,Source:EventSource{Component:endpoint-controller,Host:,},FirstTimestamp:2024-09-20 21:50:10.465517736 +0000 UTC m=+564.606924250,LastTimestamp:2024-09-20 21:50:35.390876316 +0000 UTC m=+589.506803423,Count:2,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:endpoint-controller,ReportingInstance:,}"
E0920 21:51:04.035695       1 event.go:359] "Server rejected event (will not retry!)" err="etcdserver: request timed out" logger="endpointslice-controller" event="&Event{ObjectMeta:{kube-dns.17f7123724c0e6a0  kube-system    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Service,Namespace:kube-system,Name:kube-dns,UID:f7061c97-bc19-43b6-8ede-6eb31cdeef49,APIVersion:v1,ResourceVersion:294,FieldPath:,},Reason:FailedToUpdateEndpointSlices,Message:Error updating Endpoint Slices for Service kube-system/kube-dns: failed to update kube-dns-vn94r EndpointSlice for Service kube-system/kube-dns: etcdserver: request timed out,Source:EventSource{Component:endpoint-slice-controller,Host:,},FirstTimestamp:2024-09-20 21:50:10.168997536 +0000 UTC m=+564.310404090,LastTimestamp:2024-09-20 21:50:34.380080838 +0000 UTC m=+588.496007895,Count:2,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:endpoint-slice-controller,ReportingInstance:,}"
I0920 21:51:05.923881       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="1m6.098552894s"
I0920 21:51:05.931808       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="6.913936ms"
E0920 21:51:07.439485       1 node_lifecycle_controller.go:758] "Unhandled Error" err="unable to mark all pods NotReady on node minikube: [Timeout: request did not complete within requested timeout - context deadline exceeded, etcdserver: request timed out]; queuing for retry" logger="UnhandledError"
I0920 21:51:07.445678       1 node_lifecycle_controller.go:1036] "Controller detected that all Nodes are not-Ready. Entering master disruption mode" logger="node-lifecycle-controller"
I0920 21:51:37.762954       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0920 21:51:40.035595       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0920 21:51:42.681987       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0920 21:51:42.745430       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="4.460667ms"
I0920 21:51:49.663632       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0920 21:51:50.242333       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0920 21:51:50.351250       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="598.590597ms"
I0920 21:51:50.436638       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="97.763µs"
I0920 21:51:53.450628       1 node_lifecycle_controller.go:1055] "Controller detected that some Nodes are Ready. Exiting master disruption mode" logger="node-lifecycle-controller"
I0920 21:53:44.625999       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-5bfc646b54" duration="9.59955866s"
I0920 21:53:44.937244       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-5bfc646b54" duration="209.170956ms"
I0920 21:53:44.945580       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-5bfc646b54" duration="761.33µs"
I0920 21:53:45.531685       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-5bfc646b54" duration="4.907583ms"
I0920 21:53:46.020221       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-5bfc646b54" duration="1.159358ms"
I0920 21:53:46.121168       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-5bfc646b54" duration="452.616µs"
I0920 21:53:46.934770       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-5bfc646b54" duration="932.401µs"
I0920 21:53:47.027122       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-5bfc646b54" duration="67.812µs"
I0920 21:53:47.150510       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-5bfc646b54" duration="55.977µs"
I0920 21:53:58.537214       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nodeapp-deployment-78c6d49c4b" duration="604.669445ms"
I0920 21:53:58.942359       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nodeapp-deployment-78c6d49c4b" duration="404.935246ms"
I0920 21:53:58.943040       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nodeapp-deployment-78c6d49c4b" duration="510.337µs"
I0920 21:57:00.968896       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0920 21:59:19.823407       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-5bfc646b54" duration="625.412992ms"
I0920 21:59:19.842263       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-5bfc646b54" duration="4.571169ms"
I0920 21:59:22.324647       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-5bfc646b54" duration="285.650242ms"
I0920 21:59:22.329537       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-5bfc646b54" duration="4.434273ms"
I0920 21:59:23.933001       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-5bfc646b54" duration="280.780508ms"
I0920 21:59:23.939415       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-5bfc646b54" duration="816.225µs"
I0920 21:59:46.798913       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0920 22:02:08.456403       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nodeapp-deployment-78c6d49c4b" duration="86.049544ms"
I0920 22:02:12.709543       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nodeapp-deployment-78c6d49c4b" duration="3.770836ms"
I0920 22:02:14.488174       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nodeapp-deployment-78c6d49c4b" duration="178.685µs"
I0920 22:02:18.978781       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0920 22:02:28.637714       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nodeapp-deployment-78c6d49c4b" duration="3.749055ms"
I0920 22:02:40.955775       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nodeapp-deployment-78c6d49c4b" duration="1.922725ms"
I0920 22:02:55.641888       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nodeapp-deployment-78c6d49c4b" duration="13.442079ms"
I0920 22:03:09.177951       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nodeapp-deployment-78c6d49c4b" duration="160.704µs"
I0920 22:03:47.525143       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nodeapp-deployment-78c6d49c4b" duration="4.241679ms"
I0920 22:04:02.987346       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nodeapp-deployment-78c6d49c4b" duration="714.68µs"
I0920 22:05:12.988307       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nodeapp-deployment-78c6d49c4b" duration="27.696278ms"
I0920 22:05:13.970647       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nodeapp-deployment-78c6d49c4b" duration="179.77µs"


==> kube-proxy [10e4cfb6235a] <==
E0920 21:42:28.482885       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0920 21:42:28.504522       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0920 21:42:28.792156       1 server_linux.go:66] "Using iptables proxy"
I0920 21:42:32.400642       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0920 21:42:32.401339       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0920 21:42:32.981636       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0920 21:42:32.981994       1 server_linux.go:169] "Using iptables Proxier"
I0920 21:42:33.005394       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0920 21:42:33.042494       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0920 21:42:33.081588       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0920 21:42:33.082400       1 server.go:483] "Version info" version="v1.31.0"
I0920 21:42:33.082618       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0920 21:42:33.122632       1 config.go:197] "Starting service config controller"
I0920 21:42:33.122828       1 shared_informer.go:313] Waiting for caches to sync for service config
I0920 21:42:33.122954       1 config.go:104] "Starting endpoint slice config controller"
I0920 21:42:33.122966       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0920 21:42:33.126536       1 config.go:326] "Starting node config controller"
I0920 21:42:33.126571       1 shared_informer.go:313] Waiting for caches to sync for node config
I0920 21:42:33.224052       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0920 21:42:33.224162       1 shared_informer.go:320] Caches are synced for service config
I0920 21:42:33.227890       1 shared_informer.go:320] Caches are synced for node config
W0920 21:49:24.747727       1 reflector.go:484] k8s.io/client-go/informers/factory.go:160: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0920 21:49:24.726727       1 reflector.go:484] k8s.io/client-go/informers/factory.go:160: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0920 21:49:20.617706       1 iptables.go:589] "Could not check for iptables canary" err="exit status 4" table="mangle" chain="KUBE-PROXY-CANARY"
W0920 21:49:43.959174       1 reflector.go:484] k8s.io/client-go/informers/factory.go:160: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding


==> kube-scheduler [e505a0fea491] <==
E0920 21:39:58.579592       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&limit=500&resourceVersion=0\": net/http: TLS handshake timeout" logger="UnhandledError"
W0920 21:39:58.688110       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
E0920 21:39:58.689030       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get \"https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0\": net/http: TLS handshake timeout" logger="UnhandledError"
W0920 21:39:59.187912       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": net/http: TLS handshake timeout
E0920 21:39:59.188226       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get \"https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0\": net/http: TLS handshake timeout" logger="UnhandledError"
W0920 21:40:00.497466       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": net/http: TLS handshake timeout
E0920 21:40:00.497566       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get \"https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0\": net/http: TLS handshake timeout" logger="UnhandledError"
W0920 21:40:07.369158       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": net/http: TLS handshake timeout
E0920 21:40:07.369685       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0\": net/http: TLS handshake timeout" logger="UnhandledError"
W0920 21:40:09.088244       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0": net/http: TLS handshake timeout
E0920 21:40:09.090875       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: Get \"https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0\": net/http: TLS handshake timeout" logger="UnhandledError"
W0920 21:40:10.488339       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
E0920 21:40:10.488487       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get \"https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0\": net/http: TLS handshake timeout" logger="UnhandledError"
W0920 21:40:14.194337       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0920 21:40:14.194538       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0920 21:40:14.195438       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0920 21:40:14.195534       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0920 21:40:14.195693       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0920 21:40:14.194427       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0920 21:40:14.196039       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0920 21:40:14.196686       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0920 21:40:14.198609       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0920 21:40:14.198687       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0920 21:40:14.198819       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0920 21:40:14.198838       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0920 21:40:14.199403       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0920 21:40:14.199475       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0920 21:40:14.201040       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0920 21:40:14.201518       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0920 21:40:14.201576       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E0920 21:40:14.201575       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0920 21:40:14.208198       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0920 21:40:14.208271       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0920 21:40:14.287861       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0920 21:40:14.287945       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0920 21:40:17.894017       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0920 21:40:17.894174       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0920 21:40:17.901099       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0920 21:40:17.901159       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0920 21:40:17.906097       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0920 21:40:17.988113       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0920 21:40:21.006640       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0920 21:40:21.006745       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0920 21:40:27.760866       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0920 21:40:27.760937       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0920 21:40:27.763824       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0920 21:40:27.763916       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0920 21:40:30.527244       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0920 21:40:30.527337       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0920 21:40:32.645029       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0920 21:40:32.645221       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0920 21:40:32.652489       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0920 21:40:32.652594       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0920 21:40:32.663859       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0920 21:40:32.663929       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0920 21:40:34.083568       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0920 21:40:34.083678       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0920 21:40:34.203424       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0920 21:40:34.207706       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
I0920 21:40:43.654886       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Sep 20 21:56:09 minikube kubelet[2706]: E0920 21:56:09.290150    2706 kubelet.go:2512] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="2.992s"
Sep 20 21:56:23 minikube kubelet[2706]: E0920 21:56:23.586220    2706 kubelet.go:2512] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.392s"
Sep 20 21:56:36 minikube kubelet[2706]: E0920 21:56:36.439408    2706 kubelet.go:2512] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.551s"
Sep 20 21:58:58 minikube kubelet[2706]: E0920 21:58:58.238788    2706 kubelet.go:2512] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.1s"
Sep 20 21:59:05 minikube kubelet[2706]: E0920 21:59:05.329589    2706 kubelet.go:2512] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="2.394s"
Sep 20 21:59:18 minikube kubelet[2706]: I0920 21:59:18.921560    2706 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/mongo-deployment-5bfc646b54-kh8jr" podStartSLOduration=62.096372583 podStartE2EDuration="5m37.855627415s" podCreationTimestamp="2024-09-20 21:53:41 +0000 UTC" firstStartedPulling="2024-09-20 21:54:38.151082441 +0000 UTC m=+825.275748035" lastFinishedPulling="2024-09-20 21:59:14.218488578 +0000 UTC m=+1101.035002867" observedRunningTime="2024-09-20 21:59:18.832052597 +0000 UTC m=+1105.648566886" watchObservedRunningTime="2024-09-20 21:59:18.855627415 +0000 UTC m=+1105.672141684"
Sep 20 21:59:22 minikube kubelet[2706]: I0920 21:59:22.031435    2706 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/mongo-deployment-5bfc646b54-kgl49" podStartSLOduration=63.411972288 podStartE2EDuration="5m43.031397011s" podCreationTimestamp="2024-09-20 21:53:39 +0000 UTC" firstStartedPulling="2024-09-20 21:54:38.151090788 +0000 UTC m=+825.275756373" lastFinishedPulling="2024-09-20 21:59:18.078666807 +0000 UTC m=+1104.895181096" observedRunningTime="2024-09-20 21:59:22.027650936 +0000 UTC m=+1108.844165225" watchObservedRunningTime="2024-09-20 21:59:22.031397011 +0000 UTC m=+1108.847911290"
Sep 20 21:59:23 minikube kubelet[2706]: I0920 21:59:23.639187    2706 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/mongo-deployment-5bfc646b54-72859" podStartSLOduration=61.651514143 podStartE2EDuration="5m43.639152834s" podCreationTimestamp="2024-09-20 21:53:40 +0000 UTC" firstStartedPulling="2024-09-20 21:54:38.16995766 +0000 UTC m=+825.294623245" lastFinishedPulling="2024-09-20 21:59:20.465747657 +0000 UTC m=+1107.282261936" observedRunningTime="2024-09-20 21:59:23.518617848 +0000 UTC m=+1110.335132147" watchObservedRunningTime="2024-09-20 21:59:23.639152834 +0000 UTC m=+1110.455667113"
Sep 20 22:00:09 minikube kubelet[2706]: E0920 22:00:09.324540    2706 kubelet.go:2512] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="2.108s"
Sep 20 22:00:44 minikube kubelet[2706]: E0920 22:00:44.453267    2706 kubelet.go:2512] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.49s"
Sep 20 22:01:27 minikube kubelet[2706]: E0920 22:01:27.186112    2706 kubelet.go:2512] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="2.027s"
Sep 20 22:01:34 minikube kubelet[2706]: E0920 22:01:34.975357    2706 kubelet.go:2512] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.6s"
Sep 20 22:01:58 minikube kubelet[2706]: E0920 22:01:58.361938    2706 kubelet.go:2512] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.198s"
Sep 20 22:02:00 minikube kubelet[2706]: E0920 22:02:00.462375    2706 kubelet.go:2512] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.456s"
Sep 20 22:02:03 minikube kubelet[2706]: E0920 22:02:03.701951    2706 kubelet.go:2512] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.825s"
Sep 20 22:02:05 minikube kubelet[2706]: E0920 22:02:05.655046    2706 kubelet.go:2512] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.953s"
Sep 20 22:02:08 minikube kubelet[2706]: I0920 22:02:08.120930    2706 scope.go:117] "RemoveContainer" containerID="f9c6aa88cfc5a2abaf0cd0c0f69eb19b71ee364b3cd2c7b58101c2c935e95bd1"
Sep 20 22:02:12 minikube kubelet[2706]: I0920 22:02:12.576341    2706 scope.go:117] "RemoveContainer" containerID="c9c8492a4ee875fc3b8051977f15e7b13673e3053d76f26830f2b8c0a6ebd8b9"
Sep 20 22:02:12 minikube kubelet[2706]: I0920 22:02:12.576825    2706 scope.go:117] "RemoveContainer" containerID="f9c6aa88cfc5a2abaf0cd0c0f69eb19b71ee364b3cd2c7b58101c2c935e95bd1"
Sep 20 22:02:12 minikube kubelet[2706]: E0920 22:02:12.579548    2706 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodeserver\" with CrashLoopBackOff: \"back-off 10s restarting failed container=nodeserver pod=nodeapp-deployment-78c6d49c4b-jx98h_default(6c53f76d-1bcc-4ac1-891a-5faa32b87cf5)\"" pod="default/nodeapp-deployment-78c6d49c4b-jx98h" podUID="6c53f76d-1bcc-4ac1-891a-5faa32b87cf5"
Sep 20 22:02:14 minikube kubelet[2706]: I0920 22:02:14.353574    2706 scope.go:117] "RemoveContainer" containerID="c9c8492a4ee875fc3b8051977f15e7b13673e3053d76f26830f2b8c0a6ebd8b9"
Sep 20 22:02:14 minikube kubelet[2706]: E0920 22:02:14.355737    2706 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodeserver\" with CrashLoopBackOff: \"back-off 10s restarting failed container=nodeserver pod=nodeapp-deployment-78c6d49c4b-jx98h_default(6c53f76d-1bcc-4ac1-891a-5faa32b87cf5)\"" pod="default/nodeapp-deployment-78c6d49c4b-jx98h" podUID="6c53f76d-1bcc-4ac1-891a-5faa32b87cf5"
Sep 20 22:02:25 minikube kubelet[2706]: I0920 22:02:25.968684    2706 scope.go:117] "RemoveContainer" containerID="c9c8492a4ee875fc3b8051977f15e7b13673e3053d76f26830f2b8c0a6ebd8b9"
Sep 20 22:02:28 minikube kubelet[2706]: I0920 22:02:28.541498    2706 scope.go:117] "RemoveContainer" containerID="c9c8492a4ee875fc3b8051977f15e7b13673e3053d76f26830f2b8c0a6ebd8b9"
Sep 20 22:02:28 minikube kubelet[2706]: I0920 22:02:28.541928    2706 scope.go:117] "RemoveContainer" containerID="0ddb78d83accfa119b9094bcbc499a10e3f6c757d6bf6ba708e063b1b4150166"
Sep 20 22:02:28 minikube kubelet[2706]: E0920 22:02:28.542251    2706 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodeserver\" with CrashLoopBackOff: \"back-off 20s restarting failed container=nodeserver pod=nodeapp-deployment-78c6d49c4b-jx98h_default(6c53f76d-1bcc-4ac1-891a-5faa32b87cf5)\"" pod="default/nodeapp-deployment-78c6d49c4b-jx98h" podUID="6c53f76d-1bcc-4ac1-891a-5faa32b87cf5"
Sep 20 22:02:39 minikube kubelet[2706]: I0920 22:02:39.964716    2706 scope.go:117] "RemoveContainer" containerID="0ddb78d83accfa119b9094bcbc499a10e3f6c757d6bf6ba708e063b1b4150166"
Sep 20 22:02:39 minikube kubelet[2706]: E0920 22:02:39.965074    2706 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodeserver\" with CrashLoopBackOff: \"back-off 20s restarting failed container=nodeserver pod=nodeapp-deployment-78c6d49c4b-jx98h_default(6c53f76d-1bcc-4ac1-891a-5faa32b87cf5)\"" pod="default/nodeapp-deployment-78c6d49c4b-jx98h" podUID="6c53f76d-1bcc-4ac1-891a-5faa32b87cf5"
Sep 20 22:02:52 minikube kubelet[2706]: I0920 22:02:52.965175    2706 scope.go:117] "RemoveContainer" containerID="0ddb78d83accfa119b9094bcbc499a10e3f6c757d6bf6ba708e063b1b4150166"
Sep 20 22:02:55 minikube kubelet[2706]: I0920 22:02:55.533171    2706 scope.go:117] "RemoveContainer" containerID="0ddb78d83accfa119b9094bcbc499a10e3f6c757d6bf6ba708e063b1b4150166"
Sep 20 22:02:55 minikube kubelet[2706]: I0920 22:02:55.533568    2706 scope.go:117] "RemoveContainer" containerID="9e07dccaf18186fcaaca87983f97cbad644caf8a3e03545317db512edc74a81a"
Sep 20 22:02:55 minikube kubelet[2706]: E0920 22:02:55.533795    2706 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodeserver\" with CrashLoopBackOff: \"back-off 40s restarting failed container=nodeserver pod=nodeapp-deployment-78c6d49c4b-jx98h_default(6c53f76d-1bcc-4ac1-891a-5faa32b87cf5)\"" pod="default/nodeapp-deployment-78c6d49c4b-jx98h" podUID="6c53f76d-1bcc-4ac1-891a-5faa32b87cf5"
Sep 20 22:03:08 minikube kubelet[2706]: I0920 22:03:08.962729    2706 scope.go:117] "RemoveContainer" containerID="9e07dccaf18186fcaaca87983f97cbad644caf8a3e03545317db512edc74a81a"
Sep 20 22:03:08 minikube kubelet[2706]: E0920 22:03:08.963061    2706 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodeserver\" with CrashLoopBackOff: \"back-off 40s restarting failed container=nodeserver pod=nodeapp-deployment-78c6d49c4b-jx98h_default(6c53f76d-1bcc-4ac1-891a-5faa32b87cf5)\"" pod="default/nodeapp-deployment-78c6d49c4b-jx98h" podUID="6c53f76d-1bcc-4ac1-891a-5faa32b87cf5"
Sep 20 22:03:20 minikube kubelet[2706]: I0920 22:03:20.963701    2706 scope.go:117] "RemoveContainer" containerID="9e07dccaf18186fcaaca87983f97cbad644caf8a3e03545317db512edc74a81a"
Sep 20 22:03:20 minikube kubelet[2706]: E0920 22:03:20.965380    2706 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodeserver\" with CrashLoopBackOff: \"back-off 40s restarting failed container=nodeserver pod=nodeapp-deployment-78c6d49c4b-jx98h_default(6c53f76d-1bcc-4ac1-891a-5faa32b87cf5)\"" pod="default/nodeapp-deployment-78c6d49c4b-jx98h" podUID="6c53f76d-1bcc-4ac1-891a-5faa32b87cf5"
Sep 20 22:03:32 minikube kubelet[2706]: I0920 22:03:32.960315    2706 scope.go:117] "RemoveContainer" containerID="9e07dccaf18186fcaaca87983f97cbad644caf8a3e03545317db512edc74a81a"
Sep 20 22:03:32 minikube kubelet[2706]: E0920 22:03:32.960679    2706 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodeserver\" with CrashLoopBackOff: \"back-off 40s restarting failed container=nodeserver pod=nodeapp-deployment-78c6d49c4b-jx98h_default(6c53f76d-1bcc-4ac1-891a-5faa32b87cf5)\"" pod="default/nodeapp-deployment-78c6d49c4b-jx98h" podUID="6c53f76d-1bcc-4ac1-891a-5faa32b87cf5"
Sep 20 22:03:43 minikube kubelet[2706]: I0920 22:03:43.961013    2706 scope.go:117] "RemoveContainer" containerID="9e07dccaf18186fcaaca87983f97cbad644caf8a3e03545317db512edc74a81a"
Sep 20 22:03:47 minikube kubelet[2706]: I0920 22:03:47.452930    2706 scope.go:117] "RemoveContainer" containerID="9e07dccaf18186fcaaca87983f97cbad644caf8a3e03545317db512edc74a81a"
Sep 20 22:03:47 minikube kubelet[2706]: I0920 22:03:47.453454    2706 scope.go:117] "RemoveContainer" containerID="f22726f424b1be7e9d7f13f91f25b17616e1a22968735c5857ffba53cecc58ec"
Sep 20 22:03:47 minikube kubelet[2706]: E0920 22:03:47.453663    2706 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodeserver\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=nodeserver pod=nodeapp-deployment-78c6d49c4b-jx98h_default(6c53f76d-1bcc-4ac1-891a-5faa32b87cf5)\"" pod="default/nodeapp-deployment-78c6d49c4b-jx98h" podUID="6c53f76d-1bcc-4ac1-891a-5faa32b87cf5"
Sep 20 22:04:02 minikube kubelet[2706]: I0920 22:04:02.959951    2706 scope.go:117] "RemoveContainer" containerID="f22726f424b1be7e9d7f13f91f25b17616e1a22968735c5857ffba53cecc58ec"
Sep 20 22:04:02 minikube kubelet[2706]: E0920 22:04:02.960177    2706 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodeserver\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=nodeserver pod=nodeapp-deployment-78c6d49c4b-jx98h_default(6c53f76d-1bcc-4ac1-891a-5faa32b87cf5)\"" pod="default/nodeapp-deployment-78c6d49c4b-jx98h" podUID="6c53f76d-1bcc-4ac1-891a-5faa32b87cf5"
Sep 20 22:04:15 minikube kubelet[2706]: I0920 22:04:15.959851    2706 scope.go:117] "RemoveContainer" containerID="f22726f424b1be7e9d7f13f91f25b17616e1a22968735c5857ffba53cecc58ec"
Sep 20 22:04:15 minikube kubelet[2706]: E0920 22:04:15.960140    2706 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodeserver\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=nodeserver pod=nodeapp-deployment-78c6d49c4b-jx98h_default(6c53f76d-1bcc-4ac1-891a-5faa32b87cf5)\"" pod="default/nodeapp-deployment-78c6d49c4b-jx98h" podUID="6c53f76d-1bcc-4ac1-891a-5faa32b87cf5"
Sep 20 22:04:26 minikube kubelet[2706]: I0920 22:04:26.959601    2706 scope.go:117] "RemoveContainer" containerID="f22726f424b1be7e9d7f13f91f25b17616e1a22968735c5857ffba53cecc58ec"
Sep 20 22:04:26 minikube kubelet[2706]: E0920 22:04:26.960739    2706 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodeserver\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=nodeserver pod=nodeapp-deployment-78c6d49c4b-jx98h_default(6c53f76d-1bcc-4ac1-891a-5faa32b87cf5)\"" pod="default/nodeapp-deployment-78c6d49c4b-jx98h" podUID="6c53f76d-1bcc-4ac1-891a-5faa32b87cf5"
Sep 20 22:04:41 minikube kubelet[2706]: I0920 22:04:41.953873    2706 scope.go:117] "RemoveContainer" containerID="f22726f424b1be7e9d7f13f91f25b17616e1a22968735c5857ffba53cecc58ec"
Sep 20 22:04:41 minikube kubelet[2706]: E0920 22:04:41.954125    2706 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodeserver\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=nodeserver pod=nodeapp-deployment-78c6d49c4b-jx98h_default(6c53f76d-1bcc-4ac1-891a-5faa32b87cf5)\"" pod="default/nodeapp-deployment-78c6d49c4b-jx98h" podUID="6c53f76d-1bcc-4ac1-891a-5faa32b87cf5"
Sep 20 22:04:54 minikube kubelet[2706]: I0920 22:04:54.954251    2706 scope.go:117] "RemoveContainer" containerID="f22726f424b1be7e9d7f13f91f25b17616e1a22968735c5857ffba53cecc58ec"
Sep 20 22:04:54 minikube kubelet[2706]: E0920 22:04:54.954638    2706 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodeserver\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=nodeserver pod=nodeapp-deployment-78c6d49c4b-jx98h_default(6c53f76d-1bcc-4ac1-891a-5faa32b87cf5)\"" pod="default/nodeapp-deployment-78c6d49c4b-jx98h" podUID="6c53f76d-1bcc-4ac1-891a-5faa32b87cf5"
Sep 20 22:05:08 minikube kubelet[2706]: I0920 22:05:08.953455    2706 scope.go:117] "RemoveContainer" containerID="f22726f424b1be7e9d7f13f91f25b17616e1a22968735c5857ffba53cecc58ec"
Sep 20 22:05:12 minikube kubelet[2706]: I0920 22:05:12.848062    2706 scope.go:117] "RemoveContainer" containerID="4b38dbbba85fd0676a578ac2bacb0a932500e15a6eddfe50a4f7981859121088"
Sep 20 22:05:12 minikube kubelet[2706]: E0920 22:05:12.848429    2706 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodeserver\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=nodeserver pod=nodeapp-deployment-78c6d49c4b-jx98h_default(6c53f76d-1bcc-4ac1-891a-5faa32b87cf5)\"" pod="default/nodeapp-deployment-78c6d49c4b-jx98h" podUID="6c53f76d-1bcc-4ac1-891a-5faa32b87cf5"
Sep 20 22:05:13 minikube kubelet[2706]: I0920 22:05:13.887390    2706 scope.go:117] "RemoveContainer" containerID="f22726f424b1be7e9d7f13f91f25b17616e1a22968735c5857ffba53cecc58ec"
Sep 20 22:05:13 minikube kubelet[2706]: I0920 22:05:13.887871    2706 scope.go:117] "RemoveContainer" containerID="4b38dbbba85fd0676a578ac2bacb0a932500e15a6eddfe50a4f7981859121088"
Sep 20 22:05:13 minikube kubelet[2706]: E0920 22:05:13.888130    2706 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodeserver\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=nodeserver pod=nodeapp-deployment-78c6d49c4b-jx98h_default(6c53f76d-1bcc-4ac1-891a-5faa32b87cf5)\"" pod="default/nodeapp-deployment-78c6d49c4b-jx98h" podUID="6c53f76d-1bcc-4ac1-891a-5faa32b87cf5"
Sep 20 22:05:27 minikube kubelet[2706]: I0920 22:05:27.952451    2706 scope.go:117] "RemoveContainer" containerID="4b38dbbba85fd0676a578ac2bacb0a932500e15a6eddfe50a4f7981859121088"
Sep 20 22:05:27 minikube kubelet[2706]: E0920 22:05:27.952795    2706 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodeserver\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=nodeserver pod=nodeapp-deployment-78c6d49c4b-jx98h_default(6c53f76d-1bcc-4ac1-891a-5faa32b87cf5)\"" pod="default/nodeapp-deployment-78c6d49c4b-jx98h" podUID="6c53f76d-1bcc-4ac1-891a-5faa32b87cf5"


==> storage-provisioner [781034f8b8b8] <==
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc000124440)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x5f
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc000124440, 0x18b3d60, 0xc00018de60, 0x1, 0xc000180420)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x9b
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc000124440, 0x3b9aca00, 0x0, 0x1, 0xc000180420)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc000124440, 0x3b9aca00, 0xc000180420)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:880 +0x4af

goroutine 47 [sync.Cond.Wait, 5 minutes]:
sync.runtime_notifyListWait(0xc000046350, 0x0)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0xc000046340)
	/usr/local/go/src/sync/cond.go:56 +0x99
k8s.io/client-go/util/workqueue.(*Type).Get(0xc0005865a0, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/util/workqueue/queue.go:145 +0x89
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).processNextVolumeWorkItem(0xc00045cc80, 0x18e5530, 0xc0000ba0c0, 0x203000)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:990 +0x3e
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).runVolumeWorker(...)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:929
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.3()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x5c
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc000124460)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x5f
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc000124460, 0x18b3d60, 0xc00018d4a0, 0x1, 0xc000180420)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x9b
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc000124460, 0x3b9aca00, 0x0, 0x1, 0xc000180420)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc000124460, 0x3b9aca00, 0xc000180420)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x3d6

goroutine 77 [semacquire]:
sync.runtime_SemacquireMutex(0xc0004ec1d4, 0x500, 0x1)
	/usr/local/go/src/runtime/sema.go:71 +0x47
sync.(*Mutex).lockSlow(0xc0004ec1d0)
	/usr/local/go/src/sync/mutex.go:138 +0x105
sync.(*Mutex).Lock(...)
	/usr/local/go/src/sync/mutex.go:81
golang.org/x/net/http2.transportResponseBody.Read(0xc0004e9340, 0xc000278601, 0x5ff, 0x5ff, 0x9f, 0x0, 0x0)
	/Users/medya/go/pkg/mod/golang.org/x/net@v0.0.0-20201224014010-6772e930b67b/http2/transport.go:2131 +0x567
encoding/json.(*Decoder).refill(0xc0003d6160, 0xa, 0x9)
	/usr/local/go/src/encoding/json/stream.go:165 +0xeb
encoding/json.(*Decoder).readValue(0xc0003d6160, 0x0, 0x0, 0x152aee0)
	/usr/local/go/src/encoding/json/stream.go:140 +0x1ff
encoding/json.(*Decoder).Decode(0xc0003d6160, 0x154a160, 0xc0000987f8, 0x203000, 0x203000)
	/usr/local/go/src/encoding/json/stream.go:63 +0x7c
k8s.io/apimachinery/pkg/util/framer.(*jsonFrameReader).Read(0xc0002082a0, 0xc0001f6400, 0x400, 0x400, 0x40, 0x38, 0x15b0440)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/framer/framer.go:152 +0x1a8
k8s.io/apimachinery/pkg/runtime/serializer/streaming.(*decoder).Decode(0xc0004d03c0, 0x0, 0x18bc168, 0xc0000467c0, 0x0, 0x0, 0x461dc0, 0xc0000cc000, 0xc000787e50)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/runtime/serializer/streaming/streaming.go:77 +0x89
k8s.io/client-go/rest/watch.(*Decoder).Decode(0xc0004c0000, 0xc000787ef0, 0x8, 0x18baa20, 0xc0001cea00, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/rest/watch/decoder.go:49 +0x6e
k8s.io/apimachinery/pkg/watch.(*StreamWatcher).receive(0xc0001af040)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:104 +0x14a
created by k8s.io/apimachinery/pkg/watch.NewStreamWatcher
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:71 +0xbe


==> storage-provisioner [d7edc2b35d55] <==
I0920 21:51:54.826291       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0920 21:51:55.497193       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0920 21:51:55.505948       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0920 21:52:13.415159       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0920 21:52:13.416306       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_d80c3a88-25dd-490d-8d76-cde8b274ff20!
I0920 21:52:13.417634       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"1bc72775-4368-4ad5-863c-c9ff6fc1c77d", APIVersion:"v1", ResourceVersion:"826", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_d80c3a88-25dd-490d-8d76-cde8b274ff20 became leader
I0920 21:52:13.719220       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_d80c3a88-25dd-490d-8d76-cde8b274ff20!

